
@inproceedings{bulucParallelSparseMatrixvector2009,
  title = {Parallel Sparse Matrix-Vector and Matrix-Transpose-Vector Multiplication Using Compressed Sparse Blocks},
  booktitle = {Proceedings of the Twenty-First Annual Symposium on {{Parallelism}} in Algorithms and Architectures - {{SPAA}} '09},
  author = {Buluç, Aydin and Fineman, Jeremy T. and Frigo, Matteo and Gilbert, John R. and Leiserson, Charles E.},
  date = {2009},
  pages = {233},
  publisher = {{ACM Press}},
  location = {{Calgary, AB, Canada}},
  doi = {10.1145/1583991.1584053},
  url = {http://portal.acm.org/citation.cfm?doid=1583991.1584053},
  urldate = {2020-08-07},
  abstract = {This paper introduces a storage format for sparse matrices, called compressed sparse blocks (CSB), which allows both Ax and ATx to be computed efficiently in parallel, where A is an n × n sparse r(mictrahittmriicxsaulw-spietahtΘhn(nnleznn≥zg)tnwh)on, rokyniz(eseledroriinsaglanraudnpxnaiirnsaglaltedilmeisnems)eaonnf-dvΘeΘc((tn√onr.nz /lOg√unrn)alsglpgnao)n-, which is amply high for virtually any large matrix. The storage requirement for CSB is esssentially the same as that for the morestandard compressed-sparse-rows (CSR) format, for which computing Ax in parallel is easy but ATx is difficult. Benchmark results indicate that on one processor, the CSB algorithms for Ax and ATx run just as fast as the CSR algorithm for Ax, but the CSB algorithms also scale up linearly with processors until limited by offchip memory bandwidth.},
  eventtitle = {The Twenty-First Annual Symposium},
  file = {/home/ryan/Zotero/storage/XIQASNAW/Buluç et al. - 2009 - Parallel sparse matrix-vector and matrix-transpose.pdf},
  isbn = {978-1-60558-606-9},
  langid = {english}
}

@article{kamvarAdaptiveMethodsComputation2004,
  title = {Adaptive Methods for the Computation of {{PageRank}}},
  author = {Kamvar, Sepandar and Haveliwala, Taher and Golub, Gene},
  date = {2004-07-15},
  journaltitle = {Linear Algebra and its Applications},
  volume = {386},
  pages = {51--65},
  issn = {0024-3795},
  doi = {10.1016/j.laa.2003.12.008},
  url = {http://www.sciencedirect.com/science/article/pii/S0024379504000023},
  urldate = {2020-07-31},
  abstract = {We observe that the convergence patterns of pages in the PageRank algorithm have a nonuniform distribution. Specifically, many pages converge to their true PageRank quickly, while relatively few pages take a much longer time to converge. Furthermore, we observe that these slow-converging pages are generally those pages with high PageRank. We use this observation to devise a simple algorithm to speed up the computation of PageRank, in which the PageRank of pages that have converged are not recomputed at each iteration after convergence. This algorithm, which we call Adaptive PageRank, speeds up the computation of PageRank by nearly 30\%.},
  keywords = {Eigenvalue problem,PageRank,Web matrix},
  langid = {english},
  series = {Special {{Issue}} on the {{Conference}} on the {{Numerical Solution}} of {{Markov Chains}} 2003}
}

@online{PowerWalkProceedings,
  title = {Power Walk | {{Proceedings}} of the 18th {{Australasian Document Computing Symposium}}},
  url = {https://dl-acm-org.ezproxy.uws.edu.au/doi/abs/10.1145/2537734.2537749},
  urldate = {2020-08-01},
  file = {/home/ryan/Zotero/storage/FH5YZ2YM/2537734.html}
}

@book{teamSparseMatrixOperations,
  title = {6 {{Sparse Matrix Operations}} | {{Stan Functions Reference}}},
  author = {Team, Stan Development},
  url = {https://mc-stan.org/docs/2_22/functions-reference/sparse-matrices.html},
  urldate = {2020-08-07},
  abstract = {Reference for the functions defined in the Stan math library and available in the Stan programming language.},
  file = {/home/ryan/Zotero/storage/J7V6QUKX/sparse-matrices.html}
}


