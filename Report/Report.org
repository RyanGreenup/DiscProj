#+TITLE:Page Rank
:PREAMBLE:
#+OPTIONS: broken-links:auto todo:nil H:9
#+STARTUP: overview
#+AUTHOR: Ryan Greenup
#+INFOJS_OPT: view:showall toc:3
#+PLOT: title:"Citas" ind:1 deps:(3) type:2d with:histograms set:"yrange [0:]"
#+OPTIONS: tex:t
# #+TODO: TODO IN-PROGRESS WAITING DONE
#+CATEGORY: TAD
:END:
:HTML:
#+INFOJS_OPT: view:info toc:3
#+HTML_HEAD_EXTRA: <link rel="stylesheet" type="text/css" href="./resources/style.css">
# #+CSL_STYLE: /home/ryan/Templates/CSL/nature.csl
:END:
:R:
#+PROPERTY: header-args:R :session ReportDiscProj :dir ./ :eval never-export :exports both
# exports: both (or code or whatever)
# results: table (or output or whatever)
:END:
:LATEX:

#+LATEX_HEADER: \IfFileExists{./resources/style.sty}{\usepackage{./resources/style}}{}
#+LATEX_HEADER: \IfFileExists{./resources/referencing.sty}{\usepackage{./resources/referencing}}{}
#+LATEX_HEADER: \addbibresource{./resources/references.bib}
#+LATEX_HEADER: \usepackage[mode=buildnew]{standalone}
# For TexFrag Mode inlclude TikZ here
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{decorations.fractals}
#+LATEX_HEADER: \usetikzlibrary{lindenmayersystems}
:END:
** Introduction
The centrality score of a graph is a metric that measures the
importance and popularity of a vertex. [fn:: For a small graph drawn
in a way to minimise overlapping edges the centremost geometric vertex
will coincide with the highest centrality score, for example in figure
[[irreducible-example]] vertex $D$ is the vertex with the highest
frequency during a random walk]
 

The /PageRank/ method asserts that the centrality of a vertex can be
measured by the frequency of incidence with that vertex during a
random walk.

** Implementing PageRank Generally
   :PROPERTIES:
   :CUSTOM_ID: PageRank-Generally
   :END:
A graph can be expressed as an adjacency matrix \(\mathbf{A}\):

\[
A_{i,j} \in \left\{ 0,1 \right\}
\]

Where each element of the matrix indicates whether or not travel from
vertex \(j\) to vertex \(i\) is possible with a value of 1. [fn:: Some
authors define an adjacency matrix transposed (see e.g.
cite:rosenDiscreteMathematicsIts2007,AdjacencyMatrix2020a,meghabghabSearchEnginesLink2008)
this unfourtunately includes the ~igraph~ library
cite:gaborcsardiIgraphManualPages2019 but that convention will not be
followed in this paper]

During a random walk the probability of arriving at vertex \(j\) from vertex
\(i\) can similarly be described as an element of a transition probability
matrix \(\mathbf{T}_{i,j}\), this matrix can be described by the following
relationship [fn:: In this paper \(\vec{1}\) refers to a vector containing only
values of 1, the size of which should be clear from the context]:

\begin{align}
\mathbf{T} &= \mathbf{A} \mathbf{D}^{-1}_{\mathbf{A}} \label{eq:basic-trans-def} : \\
& \mathbf{D}_{\mathbf{A}} = \mathrm{diag}\left(\vec{1} \mathbf{A}\right) \label{eq:diagScaleDef}
\end{align}

The value of \(\mathbf{D}\) is such that under matrix multiplication
\(\mathbf{A} \) will have columns that sum to 1 (i.e. a /column
stochastic matrix/, see \textsection ref:definitions), for a reducible
or non-stochastic graph the definition of \(\mathbf{D}\) would need to
be adjusted to acheive this, this is discussed below [[markov]]

During the random walk, the running tally of frequencies, at the
\(i^{\mathrm{th}}\) step of the walk, can be described by a vector
\(\vec{p}\), this vector can be determined for each step by matrix
multiplication:

\begin{align}
\vec{p_{i+1}} = \mathbf{T}\vec{p_{i}} \label{eq:recurrence}
\end{align}

This relationship is a linear recurrence relation, more importantly
however it is a /Markov Chain/
[[cite:langvilleGooglePageRankScience2012][\textsection 4.4]].

Finding the Stationary point for this relationship will give a
frequency distribution for the nodes and a metric to measure the
centrality of vertices.

** Definitions
:PROPERTIES:
:CUSTOM_ID: definitions
:END:
The following definitions are used in this report [fn:: see generally [[cite:langvilleGooglePageRankScience2012][Ch. 15]] for further reading] :

- Markov Chains :: are discrete mathematical model such that future values depend only on current values [[cite:foussAlgorithmsModelsNetwork2016][\textsection 1.5]]
- Stochastic Matrices :: contain only positive values where each column sums to 1 cite:langvilleGooglePageRankScience2012,larsonElementaryLinearAlgebra1991
  + some authors use rows (see e.g. [[cite:langvilleGooglePageRankScience2012][\textsection 15.3]]), in this paper columns will be used, i.e. columns will add to one and an entry \(\mathbf{A}_{i,j} \neq 0\) will indicate that travel is permitted from vertex \(j\) to vertex \(i\).
    - /Column Stochastic/ and /Row Stochastic/ can be used to more clearly distinguish between which type of stochastic matrix is being used.
  + Many programming languages return /unit-eigenvectors/ \(\vec{x}\) such that \(\left\lvert \left\lvert \vec{x} \right\rvert \right\rvert = 1 \) as opposed to \(\mathtt{sum} \left( \vec{\mathtt{x}}\right) = 1\), so when solving for a stationary vector it can be necessary to perform \(\vec{\mathtt{p}} \leftarrow \frac{\vec{\mathtt{p}}}{\sum \vec{p}}\)
- Irreducible :: graphs have a path from from any given vertex to another vertex. [[cite:langvilleGooglePageRankScience2012][\textsection 15.2]]
  + Ergodic :: graphs are irreducible graphs with further constraints outside
    the scope of this report (see e.g.
    cite:nathanaelackermancameronfreeralexkruckmanandrehanapatelProperlyErodicStructures2017,chenEigenvaluesInequalitiesErgodic2005)
    - It is a necessary but not a sufficient condition of ergodic graphs that all vertices be reachable from any other vertices (see cite:sazProbabilityTheoryThis for a counter example.)
- Primitive Matrices :: are non-negative irreducible matrices that have only one eigenvalue on the unit circle.
  + If a matrix is primitive it will approach a limit under exponentiation [[cite:langvilleGooglePageRankScience2012][\textsection 15.2]]
- Transition Probability Matrix :: is a stochastic matrix where each column is a vector of probabilities such that \(\mathbf{T}_{i,j}\) represents the probability of travelling from vertex \(j\) to vertex \(i\) during a random walk.
  + Some Authors consider the transpose (see e.g. cite:langvilleGooglePageRankScience2012).
- Aperiodic :: Markov chains are markov chains with an irreducible and primitive transition probability matrix.
  + If the transition probability matrix is irreducible and imprimitive it is said to be a periodic Markov chain.
- Regular :: Markov Chains are regular irreducible and aperiodic.
- Sparse :: Matrices contain a majority of elements with values equal to 0 [[cite:langvilleGooglePageRankScience2012][\textsection 4.2]]
- Sparse :: Iterating the
- PageRank :: A process of measuring graph centrality by using a random walk algorithm and measuring the most frequent node
  + In the literature (see e.g. cite:guptaWTFWhoFollow2013,langvilleGooglePageRankScience2012) the Random Surfer model is usually used to refer to the introduction of a probability of travelling to any other node, this is discussed in CROSSREF

*** Notation
:PROPERTIES:
:CUSTOM_ID: notation
:END:
- \(\mathbf{A}\) :: Is the adjacency matrix of a graph
  + \(\mathbf{A}_{i,j} = 1 \) :: Indicates that \(j\) and \(i\) are adjacent vertices.
- \(\mathbf{A}\left[:,j\right]\) :: Refers to the \(j^{\mathrm{th}}\) column vector of \(\mathrm{A}\)
  + This syntax is much like /Julia/ or /Python/ but also occurs in the literature, see e.g. [[cite:golubMatrixComputations1996][\textsection 1.1.8]]
- \(\mathbf{T}\) :: Is the transition probability matrix of a graph
  + \(\mathf{T}_{i,j}\) is equal to the probability of travelling \(j \rightarrow  i \) during a random walk.
    - \(\mathbf{T} = \mathbf{A} \mathbf{D}_{\mathbf{A}}^{-1} \)
      + Where \(\mathbf{D}^{-1}\) is a matrix such that multiplication with which scales each column of \(\mathbf{A}\) to 1.
        - \(\mathbf{D}^{-1}_{\mathbf{A}} = \vec{1}\mathbf{D}^{-1}_{\mathbf{A}} = \frac{1}{\vec{1}\mathbf{D}_{\mathbf{A}}} \) for some stochastic matrix \(\mathbf{A}\)
- \(n\) :: Refers to the number of vertices in a graph elements of a matrix
  + \(n = \mathtt{nrow}\left(\mathbf{A}\right) = \mathtt{ncol}\left(\mathbf{A}\right)\)
- \(\mathbf{B}_{i,j} = \frac{1}{n}\) :: Is a matrix of size \(n\times n\) representing the background probability of uniformly selecting any vertex of a graph.
- \(\vec{1}\) :: is a vector of length \(n\) containing only the value 1.
  + The convention that a vector behaves as a vertical \(n \times 1 \) matrix will be used here.
  + Some authors use \(\mathbf{e}\), see e.g. cite:langvilleGooglePageRankScience2012
- \(\mathbf{J} = \vec{1}\cdot \vec{1}^{\mathrm{T}} \iff \mathbf{J}_{i,j} = 1\) :: Is a completely dense \(n \times n \) matrix.
  + It's worth noting that \(\mathbf{E}, \mathbf{J}\) are common choices for this matrix.
- \(\alpha\) :: The probability of teleporting from one vertex to another during a random walk.
  + In the literature $\alpha$ is often referred to as a damping factor (see e.g.  cite:berkhoutRankingNodesGeneral2018a,brinkmeierPageRankRevisited2006a,fuDampingFactorGoogle2006,kamvarAdaptiveMethodsComputation2004b,bianchiniPageRank2005)
  or a smoothing constant (see e.g cite:koppelMeasuringDirectIndirect2014).
- $\vec{p}_{i} = \frac{\mathrm{deg}(v_{1})}{\mathrm{vol}(G)}$
  + $\mathrm{vol}(G) = \sum^{n}_{i = 1} \left[ \mathrm{indeg}(v) \right] = \sum^{n}_{i = 1} \left[ \mathrm{outdeg}(v) \right ] = \sum^{n}_{i = 1} \left[ \mathrm{deg}(v) \right]$

** Random Surfer Model
*** Issues
:PROPERTIES:
:CUSTOM_ID: issues
:END:
The approach in [[#PageRank-Generally]] has the following issues

1. Convergence of eqref:eq:recurrence
   a. Will this relationship converge or diverge?
   b. How quickly will it converge?
   c. Will it converge uniquely?
2. Reducible graphs
   1. If it is not possible to perform a random walk across an entire graph for all initial conditions, this approach doesn't have a clear analogue.
3. Cycles
   1. A graph that is cyclical may not converge uniquely
      1. Consider for example the graph \(A\rightarrow B\).

*** Markov Chains
:PROPERTIES:
:CUSTOM_ID: markov
:END:
The relationship in eqref:eq:recurrence is a /Markov Chain/  and it is known
that the power method will converge: [fn::A /Markov Chain/ is
simply any process that evolves depending on it's current condition, it's
interesting to note however that the theory of /Markov Chains/ is not mentioned in any
of the original papers by page and brin
[[cite:langvilleGooglePageRankScience2012][\textsection 4.4]] ]

- for a stochastic irreducible markov chain [[cite:larsonElementaryLinearAlgebra1991][\textsection 1.5.5]],
- regardless of the initial condition of the process for an /aperiodic/ Markov chain [[cite:langvilleGooglePageRankScience2012][\textsection 4.4]]

**** Stochastic
:PROPERTIES:
:CUSTOM_ID: stochastic
:END:
If a vertex had a 0 outdegree the corresponding column sum for the adjacency
matrix describing that graph would also be zero and the matrix non-stochastic,
this could occur in the context of a random walk where a link to a page with no
outgoing links was followed (e.g. an image), this would be the end of the
walk.

So to ensure that eqref:eq:recurrence will converge, the probability transition
matrix must be made stochastic, to acheive this a uniform probability of teleporing from a dead end to any other vertex can be introduced:

\begin{align}
\mathrm{S} = \mathrm{T}+ \frac{\vec{a} \cdot \vec{1}^{\mathrm{T}} }{n} \label{eq:nearly-random-surfer}
\end{align}

This however would not be sufficient to ensure that eqref:eq:recurrence would converge, in addition the transition probability matrix must be made irreducible and aperiodic (i.e. primitive). cite:langvilleGooglePageRankScience2012

# #+ATTR_LATEX: :float wrap
#+NAME:  fig:stochastic-example
#+CAPTION: \(D\) is a /dangling node/, a dead end during a random walk, the corresponding probability transition matrix \((\mathbf{T})\) is hence non-stochastic (and also reducible), Introducing some probability of teleporting from a dead end to any other vertex as per eqref:eq:nearly-random-surfer (denoted in red) will cause \(\mathbf{T}\) to be stochastic.
#+attr_html: :width 400px
#+attr_latex: :width 6cm
[[file:media/dot/stochastic_graph_example.dot.png]]

**** Irreducible
A graph that allows travel from any given vertex to any other vertex is said to be irreducible cite:langvilleGooglePageRankScience2012, see for example figure [[irreducible-example]], this is important in the context of a random walk because only in an irreducible graph can all vertexes be reached from any initial condition.

#+NAME: irreducible-example
#+CAPTION: Example of a reducible graph, observe that although \(C\) is not a dead end as discussed in [[#stochastic]], there is no way to travel from \(C\) to \(A\), by adding an edge such an edge in the resulting graph is irreducible. The resulting graph is also aperiodic (due to the loop on \(B\)) and stochastic, so there will be a stationary distribution corresponding to eqref:eq:recurrence.
#+attr_html: :width 400px
#+attr_latex: :width 6cm
[[file:media/dot/reducible_graph_example.dot.png]]

**** Aperiodic
An a periodic graph has only one eigenvalue that lies on the unit circle, this is important because \(\lim_{k\rightarrow \infty} \left( \frac{\mathbf{A}}{r}^{k} \right) \) exists for a non-negative irreducible matrix \(\mathbf{A}\) if and only if \mathbf{A} is aperiodic. A graph that is a periodic can be made aperiodic by interlinking nodes [fn:: Actually it would be sufficient to merely link one vertex to itself [[cite:langvilleGooglePageRankScience2012][\textsection 15.2]] but this isn't very illustrative or helpful in this context ]


#+NAME: fig:aperiodic
#+CAPTION: A periodic graph with all eigenvalues on the unit circle \(\xi = \frac{\sqrt{2}}{2} e^{\frac{\pi i}{4} k}\), by adding in extra edges the graph is now aperiodic, this does not represent the random surfer model, which would in theory connect every vertex but with some probability.
#+attr_html: :width 400px
#+attr_latex: :width 9cm
[[file:media/dot/aperiodic.dot.png]]

**** The Fix
:PROPERTIES:
:CUSTOM_ID: fix
:END:
To ensure that the transition probability matrix is primitive (i.e. irreducible and aperiodic) as well as stochastic, instead of introducing the possible to teleport out of dead ends, introduce a probability of teleporting to any node at any time (\(\alpha \)), this approach is known as the /Random Surfer/ model and the transition probability matrix is given by cite:larrypageAnatomyLargescaleHypertextual1998 :

\begin{align}
\mathbf{S} = \alpha \mathbf{T} + \frac{(1- \alpha)}{n} \mathbf{J} \label{eq:random-surfer}
\end{align}

This matrix is primitive and stochastic and so will converge (it is also unfourtunately completely dense, see [[#solving-stationary-dist]] [[cite:langvilleGooglePageRankScience2012][\textsection 4.5]].

The relation ship in eqref:eq:recurrence can now be re expressed as:

\begin{align}
\vec{p_{i+1}} \rightarrow \mathbf{T} \vec{p}_{i} \label{eq:random-surfer-recurrence}
\end{align}



#+NAME: fig:rseg
#+CAPTION:  A graph that is aperiodic, reducible and non-stochastic, by applying the random surfer model eqref:eq:random-surfer blue /teleportation/ edges are introduced, these may be followed with a probability of \(1 - \alpha \)
#+attr_html: :width 400px
#+attr_latex: :width 9cm
[[file:media/dot/random_surfer.dot.png]]
*** Limitations
The /Random Surfer/ Model can only consider positively weighted edges, it cannot
take into account negatively weighted edges. This limitation is increasingly
important as techniques of sentiment analysis are developed which could indicate
that links promote aversion rather than endorsement (e.g. a negative review or
an innapropriate advertisement).
** Power walk
:PROPERTIES:
:CUSTOM_ID: pwalk
:END:
The /Power Walk/ method is an alternative approach to develop a probability
transition matrix to use in place of eqref:eq:recurrence.

Let the probability of travelling to a non-adjacent vertex be some value \(x\)
and \(\beta\) be the ratio of probability between following an edge or
teleporting to another vertex.

This transition probability matrix would be such that the probability of
travelling some vertex \(j \rightarrow i\) would be :

\begin{align}
\mathbf{W}_{i, j} = x\beta^{\mathbf{A_{i,j}}} \label{eq:prob-power-walk}
\end{align}

Where \(\mathbf{W}\) denotes the power walk probability transition matrix.

Whe probability of travelling to any given vertex must be 1 and so:


\begin{align}
      1 &= \sum^{n}_{j= 1}   \left[ x \beta^{\mathbf{A_{i,j}}} \right] \\
       \implies  x&= \left( \sum^{n}_{j= 1}   \beta^{\mathbf{A_{i,j}}}
       \right)^{-1} \label{eq:powerwalk-x-val}
\end{align}

Substituting the value of \(x\) from eqref:eq:powerwalk-x-val into eqref:prob-power-walk gives the probability as:

\begin{align}
      \mathbf{W}_{i,j} &= \frac{\beta^{\mathbf{A}__i,j}}{\sum^{n}_{i=j}
      \left[ \beta^{\mathbf{A}_{i,j}} \right] }
\end{align}

In this model all vertices are interconnected by some probability of jumping to
another vertex, so much like the random surfer model eqref:eq:random-surfer discussed
at [[#fix]] \(\mathbf{W}\) will be a primitive stochastic matrix and so if
\(\mathbf{W}\) was used in place of \(\mathbf{T}\) in eqref:eq:recurrence a solution
would exist.

** Sparse Matrices
Most Adjacency matrices resulting from webpages and analagous networks
result in sparse adjacency matrices (see figure [[fig:den_undir_ba]]),
this is a good thing because it requires far less computational
resources to work with a sparse matrix than a dense matrix
 [[cite:langvilleGooglePageRankScience2012][\textsection 4.2]] .

Sparse matrices can be expressed in alternetive forms so as to reduce the memory
footprint associated with that matrix, one such method is the /Compressed Row Storage/ method, this involves listing the elements as a table as in eqref:eq:ordinary and eqref:eq:crc.

This is implemented in */R/* with the ~Matrix~ package
cite:batesMatrixSparseDense2019a .

\begin{align}
    \begin{bmatrix}
	1 & 0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 0 & 0 \\
	0 & \phi & 0 & 0 & 0 \\
	0 & 0 & 0 & 0 & \pi \\
	0 & 0 & 0 & 0 & 0 \\
    \end{bmatrix}  \label{eq:ordinary} \\
    \ \nonumber \\
    \ \nonumber \\
    \begin{matrix}
	\mathrm{Row\ Index} & \mathrm{Col\ Index} & \mathrm{Value}\\
	1 & 1 & 1 \\
	3 & 2 & \phi \\
	4 & 5 & \pi \\
    \end{matrix}  \label{eq:crc}
\end{align}


*** Solving the Stationary Distribution
:PROPERTIES:
:CUSTOM_ID: solving-stationary-dist
:END:

The relationship in eqref:eq:recurrence [fn:: This assumes that the transition
probability matrix is stochastic and primitive as it would be for \(\mathbf{S}\)
and \(\mathbf{W}\)] is equivelant to the eigenvalue value problem, where
\(\vec{p} = \lim_{i \rightarrow \infty} \left( \vec{p_{i}}\right)\) is the
eigenvector [fn:: More accurately the eigenvector specifically scaled
specifically to 1, so it would be more correct to say the eigenvector
\(\frac{\vec{x}}{\sum \vec{x}} \) ] \( \vec{x} \) that corresponds to the
eigenvalue \(\xi=1\):

\begin{align}
\vec{p} (1) = \mathbf{S} \vec{p} \label{eq:eigenprob}
\end{align}

Solving eigenvectors for large matrices can be very resource intensive and so
this approach isn't suitable for analysing large networks.

Upon iteration eqref:eq:recurrence will converge to stable stationary point, as discussed
in [[#fix]], this approach is known as the power method
cite:larsonElementaryLinearAlgebra1991a and is what in practice must be
implemented to solve the stationary distribution of
eqref:eq:random-surfer-recurrence and eqref:eq:recurrence.


As mentioned in [[#fix]] and [[#pwalk]], the /Random Surfer/ and /Power Walk/
transtition probability matrices are completely dense, that means applying the
power method will not be able to take advantage of using sparse matrix
algorithms.

With some effort however it is possible to express the algorithms in such a way that only involves sparse matrices.

** Implementing the Models
To Implement the models, first they'll be implemented using an ordinary matrix and then improved to work with sparse matrices and algorithms, the implementation has been performed with /*R*/ and the preamble is provided in listings [[preamble]]

#+NAME: preamble
#+CAPTION: Implemented Packages used in this report
#+BEGIN_SRC R :results none
  if (require("pacman")) {
      library(pacman)
    }else{
      install.packages("pacman")
      library(pacman)
    }

    pacman::p_load(tidyverse, Matrix, igraph, plotly, mise, docstring, mise)
#+END_SRC

** Implementing the Random Surfer
*** Small Graph, Ordinary Matrices
  :PROPERTIES:
  :CUSTOM_ID: implementing-page-rank-methods
  :END:
**** Example Graph
   :PROPERTIES:
   :CUSTOM_ID: example-graph
   :END:

Consider the following graph:

#+BEGIN_SRC R :exports both :results output graphics file :file media/example-graph-power-walk.png :session ReportDiscProj
g1 <- igraph::graph.formula(1++2, 1+-8, 1+-5, 2+-5, 2+-7, 2+-8, 2+-6, 2+-9, 3++4, 3+-5, 3+-6, 3+-9, 3+-10, 4+-9, 4+-10, 4+-5, 5+-8, 6+-8, 7+-8)
  plot(g1)
#+END_SRC

#+NAME: example-rs-graph
#+CAPTION: Exemplar graph to solve Random Surfer Model with
#+attr_html: :width 400px
#+attr_latex: :width 12cm
#+RESULTS[7de5d25f92fb5d3fdc637f7e5e9390e4492faabe]:
[[file:media/example-graph-power-walk.png]]

***** Adjacency Matrix
    :PROPERTIES:
    :CUSTOM_ID: adjacency-matrix
    :END:

The adjacency Matrix is given by:

#+NAME: adj-mat-random-surf
#+CAPTION: Return the Adjacency Matrix corresponding to figure [[example-rs-graph]]
#+BEGIN_SRC R :results output :session ReportDiscProj
  A <- igraph::get.adjacency(g1, names = TRUE, sparse = FALSE)

  ## igraph gives back the transpose
  (A <- t(A))
#+END_SRC

#+RESULTS[07ccd3f9e44696a6bf13432d1c4d930c65dab917]: adj-mat-random-surf
#+begin_example
   1 2 8 5 7 6 9 3 4 10
1  0 1 1 1 0 0 0 0 0  0
2  1 0 1 1 1 1 1 0 0  0
8  0 0 0 0 0 0 0 0 0  0
5  0 0 1 0 0 0 0 0 0  0
7  0 0 1 0 0 0 0 0 0  0
6  0 0 1 0 0 0 0 0 0  0
9  0 0 0 0 0 0 0 0 0  0
3  0 0 0 1 0 1 1 0 1  1
4  0 0 0 1 0 0 1 1 0  1
10 0 0 0 0 0 0 0 0 0  0
#+end_example

#+RESULTS[c1566cbc1c0c0b0dc96d2df0449ca574cf7553ea]:
#+begin_example
   1 2 8 5 7 6 9 3 4 10
1  0 1 1 1 0 0 0 0 0  0
2  1 0 1 1 1 1 1 0 0  0
8  0 0 0 0 0 0 0 0 0  0
5  0 0 1 0 0 0 0 0 0  0
7  0 0 1 0 0 0 0 0 0  0
6  0 0 1 0 0 0 0 0 0  0
9  0 0 0 0 0 0 0 0 0  0
3  0 0 0 1 0 1 1 0 1  1
4  0 0 0 1 0 0 1 1 0  1
10 0 0 0 0 0 0 0 0 0  0
#+end_example

***** Probability Transition Matrix
    :PROPERTIES:
    :CUSTOM_ID: probability-transition-matrix
    :END:

The probability transition matrix is such that each column of the
initial state distribution (i.e. the transposed adjacency matrix) is
scaled to 1.

if \(\mathbf{A}\) had vertices with a 0 out-degree, the relationship in eqref:eq:basic-trans-def would not work, instead columns that sum to 0 would
need to be left while all other columns be divided by the column sum to get
\(\mathbf{T}\). An alternative approach using sparse matrices will be presented
below and in this case there exists corresponding \(\mathbf{T}\) that is
stochastic and so it is sufficient to use the relationship at
eqref:eq:basic-trans-def, this is shown in listing [[basic-trans-def]].

#+NAME: basic-trans-def
#+CAPTION: Solve the Transition Probability Matrix by scaling each column to 1 using matrix multiplication.
#+BEGIN_SRC R
(T <- A %*% diag(1/colSums(A)))

#+END_SRC

#+RESULTS[3a75a37735656a12861ee3215b3dbcf861147b59]: basic-trans-def
| 0 | 1 | 0.2 | 0.25 | 0 |   0 |                 0 | 0 | 0 |   0 |
| 1 | 0 | 0.2 | 0.25 | 1 | 0.5 | 0.333333333333333 | 0 | 0 |   0 |
| 0 | 0 |   0 |    0 | 0 |   0 |                 0 | 0 | 0 |   0 |
| 0 | 0 | 0.2 |    0 | 0 |   0 |                 0 | 0 | 0 |   0 |
| 0 | 0 | 0.2 |    0 | 0 |   0 |                 0 | 0 | 0 |   0 |
| 0 | 0 | 0.2 |    0 | 0 |   0 |                 0 | 0 | 0 |   0 |
| 0 | 0 |   0 |    0 | 0 |   0 |                 0 | 0 | 0 |   0 |
| 0 | 0 |   0 | 0.25 | 0 | 0.5 | 0.333333333333333 | 0 | 1 | 0.5 |
| 0 | 0 |   0 | 0.25 | 0 |   0 | 0.333333333333333 | 1 | 0 | 0.5 |
| 0 | 0 |   0 |    0 | 0 |   0 |                 0 | 0 | 0 |   0 |

#+BEGIN_EXAMPLE
  ##    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]      [,9] [,10]
  ## 1     0    1    0    0 0.25  0.0    0  0.2 0.0000000   0.0
  ## 2     1    0    0    0 0.25  0.5    1  0.2 0.3333333   0.0
  ## 3     0    0    0    1 0.25  0.5    0  0.0 0.3333333   0.5
  ## 4     0    0    1    0 0.25  0.0    0  0.0 0.3333333   0.5
  ## 5     0    0    0    0 0.00  0.0    0  0.2 0.0000000   0.0
  ## 6     0    0    0    0 0.00  0.0    0  0.2 0.0000000   0.0
  ## 7     0    0    0    0 0.00  0.0    0  0.2 0.0000000   0.0
  ## 8     0    0    0    0 0.00  0.0    0  0.0 0.0000000   0.0
  ## 9     0    0    0    0 0.00  0.0    0  0.0 0.0000000   0.0
  ## 10    0    0    0    0 0.00  0.0    0  0.0 0.0000000   0.0
#+END_EXAMPLE

****** Create a Function
     :PROPERTIES:
     :CUSTOM_ID: create-a-function
     :END:

#+BEGIN_SRC R
   adj_to_probTrans <- function(A) {
     A %*% diag(1/colSums(A))
   }

   (T <- adj_to_probTrans(A)) %>% round(2)
#+END_SRC

#+RESULTS[80ac3570de54db17c9439ac1d3c0e05a3d66e0d5]:
| 0 | 1 | 0.2 | 0.25 | 0 |   0 |    0 | 0 | 0 |   0 |
| 1 | 0 | 0.2 | 0.25 | 1 | 0.5 | 0.33 | 0 | 0 |   0 |
| 0 | 0 |   0 |    0 | 0 |   0 |    0 | 0 | 0 |   0 |
| 0 | 0 | 0.2 |    0 | 0 |   0 |    0 | 0 | 0 |   0 |
| 0 | 0 | 0.2 |    0 | 0 |   0 |    0 | 0 | 0 |   0 |
| 0 | 0 | 0.2 |    0 | 0 |   0 |    0 | 0 | 0 |   0 |
| 0 | 0 |   0 |    0 | 0 |   0 |    0 | 0 | 0 |   0 |
| 0 | 0 |   0 | 0.25 | 0 | 0.5 | 0.33 | 0 | 1 | 0.5 |
| 0 | 0 |   0 | 0.25 | 0 |   0 | 0.33 | 1 | 0 | 0.5 |
| 0 | 0 |   0 |    0 | 0 |   0 |    0 | 0 | 0 |   0 |

#+BEGIN_EXAMPLE
  ##    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
  ## 1     0    1    0    0 0.25  0.0    0  0.2 0.00   0.0
  ## 2     1    0    0    0 0.25  0.5    1  0.2 0.33   0.0
  ## 3     0    0    0    1 0.25  0.5    0  0.0 0.33   0.5
  ## 4     0    0    1    0 0.25  0.0    0  0.0 0.33   0.5
  ## 5     0    0    0    0 0.00  0.0    0  0.2 0.00   0.0
  ## 6     0    0    0    0 0.00  0.0    0  0.2 0.00   0.0
  ## 7     0    0    0    0 0.00  0.0    0  0.2 0.00   0.0
  ## 8     0    0    0    0 0.00  0.0    0  0.0 0.00   0.0
  ## 9     0    0    0    0 0.00  0.0    0  0.0 0.00   0.0
  ## 10    0    0    0    0 0.00  0.0    0  0.0 0.00   0.0
#+END_EXAMPLE

**** Page Rank Random Surfer
   :PROPERTIES:
   :CUSTOM_ID: page-rank-random-surfer
   :END:

Recall from [[#fix]] the following variables of the /Random Surfer/ model:


\begin{align}
    \mathbf{B} &= \alpha T +  \left( 1- \alpha \right)B :\\
\ \\
    \mathbf{B}&= \begin{bmatrix}
    \frac{1}{n} & \frac{1}{n} & \ldots & \frac{1}{n} \\
    \frac{1}{n} & \frac{1}{n} & \ldots & \frac{1}{n} \\
        \vdots      & \vdots      & \ddots & \vdots  \\
    \frac{1}{n} & \frac{1}{n} & \ldots & \frac{1}{n} \\
    \end{bmatrix} \label{eq:bgval1} \\
    n&= \left| \left| V \right| \right| \\
    \alpha &\in [0,1]
\end{align}

These are
assigned to /*R*/ variables in listing [[r-var-random-surfer]].

#+NAME: r-var-random-surfer
#+CAPTION: Assign Random Surfer Variables, observe the unique value given to ~l~, this will be relevant later.
#+BEGIN_SRC R
  B <- matrix(rep(1/nrow(T), length.out = nrow(T)**2), nrow = nrow(T))
  l <- 0.8123456789

  (S <- l*T+(1-l)*B) %>% round(2)


#+END_SRC

#+RESULTS[a1c71764fb16e4d62521e674975bfa6aee4ae335]: r-var-random-surfer
| 0.02 | 0.83 | 0.18 | 0.22 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| 0.83 | 0.02 | 0.18 | 0.22 | 0.83 | 0.42 | 0.29 | 0.02 | 0.02 | 0.02 |
| 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| 0.02 | 0.02 | 0.18 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| 0.02 | 0.02 | 0.18 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| 0.02 | 0.02 | 0.18 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| 0.02 | 0.02 | 0.02 | 0.22 | 0.02 | 0.42 | 0.29 | 0.02 | 0.83 | 0.42 |
| 0.02 | 0.02 | 0.02 | 0.22 | 0.02 | 0.02 | 0.29 | 0.83 | 0.02 | 0.42 |
| 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
***** Eigen Value Method
    :PROPERTIES:
    :CUSTOM_ID: eigen-value-method
    :END:

The eigenvector corresponding to the the eigenvalue of 1 will be the
stationary point, this is shown in listing [[eigenSol-rand-surf]]

#+NAME: eigenSol-rand-surf
#+CAPTION: Solve the Eigen vectors and Eigen values of the transition probability matrix corresponding to the graph.
#+BEGIN_SRC R
  eigen(S, symmetric = FALSE)
#+END_SRC

#+RESULTS[adc3575f8b0eb19bbca16a4d24fedbabc2fbb026]: eigenSol-rand-surf

#+begin_example
eigen() decomposition
$values
 [1]  1.000000e+00 -8.123457e-01 -8.123457e-01  8.123457e-01 -3.407464e-09  3.407464e-09
 [7]  6.878591e-17 -4.393838e-17 -1.126771e-18 -1.292735e-32

$vectors
            [,1]          [,2]          [,3]          [,4]          [,5]          [,6]
 [1,] 0.48726141 -7.071005e-01  1.590774e-03  5.000000e-01  6.735753e-01 -6.735753e-01
 [2,] 0.52676629  7.071005e-01 -1.590774e-03  5.000000e-01  9.622504e-02 -9.622505e-02
 [3,] 0.49149620 -2.975837e-03  7.071050e-01 -5.000000e-01  9.622504e-02 -9.622505e-02
 [4,] 0.48044122  2.975837e-03 -7.071050e-01 -5.000000e-01  2.886751e-01 -2.886751e-01
 [5,] 0.04932738  1.463673e-18 -5.541166e-17  2.124631e-17 -3.849002e-01  3.849002e-01
 [6,] 0.04932738  1.463673e-18  5.541166e-17  2.124631e-17 -3.849002e-01  3.849002e-01
 [7,] 0.04932738  1.463673e-18 -2.077937e-17  2.124631e-17 -3.849002e-01  3.849002e-01
 [8,] 0.04243328 -6.484884e-18 -1.103904e-17  6.319692e-17  8.072508e-09  8.072508e-09
 [9,] 0.04243328  6.952446e-18 -9.740331e-18  6.005334e-17  8.072508e-09  8.072509e-09
[10,] 0.04243328  6.952446e-18 -9.740331e-18  6.005334e-17  8.072508e-09  8.072509e-09
               [,7]          [,8]          [,9]         [,10]
 [1,] -3.963430e-01  3.962600e-01  1.828019e-01 -1.752367e-01
 [2,] -1.291621e-01  2.027302e-01  2.199538e-01 -2.197680e-01
 [3,] -3.955284e-01  3.894308e-02  2.223048e-01 -2.248876e-01
 [4,] -4.215353e-01  1.043870e-01  2.747562e-01 -2.777266e-01
 [5,]  5.166485e-01 -8.109210e-01 -8.798152e-01  8.790721e-01
 [6,]  5.201366e-02 -1.308878e-01 -1.049028e-01  1.056778e-01
 [7,]  1.346275e-01 -1.936007e-01  9.054366e-02 -9.554811e-02
 [8,]  2.547528e-16 -1.352936e-16 -1.025353e-16  1.072771e-16
 [9,]  3.196396e-01  1.965446e-01 -2.821213e-03 -5.466313e-03
[10,]  3.196396e-01  1.965446e-01 -2.821213e-03  1.388344e-02

#+end_example

So in this case the stationary point corresponds to the eigenvector given by:
\[
\langle -0.49, -0.53, -0.49, -0.48, -0.05, -0.05, -0.05, -0.04, -0.04, -0.04 \rangle
\]

this can be verified by using identity eqref:eq:eigenprob:

$$
1 \vec{p} = S\vec{p}
$$

#+BEGIN_SRC R
  (p     <- eigen(S)$values[1] * eigen(S)$vectors[,1]) %>% Re() %>%  round(2)
#+END_SRC

#+RESULTS[8c4fc0c8edff5276b3d867f244f7b8cf82ec6570]:
| 0.49 |
| 0.53 |
| 0.04 |
| 0.05 |
| 0.05 |
| 0.05 |
| 0.04 |
| 0.49 |
| 0.48 |
| 0.04 |

#+BEGIN_SRC R
  (p_new <- S %*% p) %>% Re()  %>% as.vector() %>% round(2)
#+END_SRC

#+RESULTS[bed98d6948b27a333c70e2bb0e89e6b802f77d0d]:
| 0.49 |
| 0.53 |
| 0.04 |
| 0.05 |
| 0.05 |
| 0.05 |
| 0.04 |
| 0.49 |
| 0.48 |
| 0.04 |

However this vector does not sum to 1 so the scale should be adjusted
(for probabilities the vector should sum to 1):

#+BEGIN_SRC R
  (p_new <- p_new/sum(p_new)) %>% Re() %>% round(2)
#+END_SRC

#+RESULTS[5f23c0c6ebf01cd1a6ec983c30c7fb08b8c9e927]:
| 0.22 |
| 0.23 |
| 0.02 |
| 0.02 |
| 0.02 |
| 0.02 |
| 0.02 |
| 0.22 |
| 0.21 |
| 0.02 |

***** Power Value Method
    :PROPERTIES:
    :CUSTOM_ID: power-value-method
    :END:

Using the power method should give the same result as the eigenvalue method, again but for scale:

#+BEGIN_SRC R
  p_new <- p_new *123456789

  while (sum(round(p, 9) != round(p_new, 9))) {
      (p     <- p_new)
      (p_new <- S %*% p)
  }

  round(Re(p_new), 2)
#+END_SRC

#+RESULTS[93f1088c513e3a607eba35114723e6175a54335c]:
| 26602899.78 |
| 28759738.27 |
|  2316719.99 |
|  2693115.49 |
|  2693115.49 |
|  2693115.49 |
|  2316719.99 |
| 26834105.29 |
| 26230539.22 |
|  2316719.99 |

If scaled to 1 the
same value will be returned:

#+BEGIN_SRC R
  (p_new <- p_new/sum(p_new)) %>% Re %>% round(2)
#+END_SRC

#+RESULTS[5f23c0c6ebf01cd1a6ec983c30c7fb08b8c9e927]:
| 0.22 |
| 0.23 |
| 0.02 |
| 0.02 |
| 0.02 |
| 0.02 |
| 0.02 |
| 0.22 |
| 0.21 |
| 0.02 |

***** Scaling
    :PROPERTIES:
    :CUSTOM_ID: scaling
    :END:

If the initial state sums to 1, then the scale of the stationary
vector will also sum to 1, so this isn't in practice an issue for the power method:

#+BEGIN_SRC R
  p     <- c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0)
  p_new <- S %*% p

  while (sum(round(p, 9) != round(p_new, 9))) {
      (p     <- p_new)
      (p_new <- S %*% p)
  }

  cbind(p_new, p)
#+END_SRC

#+RESULTS[2588d1419148286a42553ef3f6471216baeecccf]:

#+BEGIN_EXAMPLE
  ##         [,1]      [,2]
  ## 1  0.2129185 0.2129185
  ## 2  0.2313481 0.2313481
  ## 3  0.2156444 0.2156444
  ## 4  0.2104889 0.2104889
  ## 5  0.0232000 0.0232000
  ## 6  0.0232000 0.0232000
  ## 7  0.0232000 0.0232000
  ## 8  0.0200000 0.0200000
  ## 9  0.0200000 0.0200000
  ## 10 0.0200000 0.0200000
#+END_EXAMPLE
*** Large Graph, Sparse Matrices using CRS
**** Creating the Probability Transition Matrix
Implementing the page rank method on a larger graph requires the use of more efficient form of matrix storage.

An adjacency matrix (atleast in the context of graphs relating to webpages and social networks) will contain elements that are mostly zero because the number of edges leaving any vertex will tend to be significantly less than the total number of vertices.

A matrix exhibiting this property is known as a sparse matrix CITE

The properties of a sparse matrix can be implemented in order to improve performance, one such method to acheive this is /Compressed Sparse Row/ (CSR) storage, which involves creating a seperate array of values and corresponding indices. CITE

This is implemented by the Matrix package in */R/*. CITE

An sparse matrix can be created using the following syntax, which will return a matrix of the class ~dgCMatrix~:

#+begin_src R :results output
library(Matrix)
## Create Example Matrix
n <- 20
m <- 10^6
i <- sample(1:m, size = n); j <- sample(1:m, size = n); x <- rpois(n, lambda = 90)
A <- sparseMatrix(i, j, x = x, dims = c(m, m))

summary(A)
#+end_src

#+RESULTS[01a1045469fde58128a6b943003aabcf4b005731]:
#+begin_example
1000000 x 1000000 sparse Matrix of class "dgCMatrix", with 20 entries
        i      j   x
1  808361  49547  90
2  428268  98950  88
3  111018 198129  96
4  330573 218343  85
5  556574 271674  94
6   80297 329285  85
7  410107 346309  88
8  893744 380886  88
9  753918 492423  93
10 309389 515236  90
11 401507 611744 104
12 110381 625266  94
13 297550 641236  97
14 648515 691630 103
15 146681 732490 103
16 821733 778479  71
17 941850 843950  73
18 105277 966175 109
19  15447 974487  89
20 941903 990749  73
#+end_example

As before in section [[#probability-transition-matrix]], the probability transition matrix can be found by:

1. Transposing the adjacency matrix, then
2. Scaling the columns to one

To implement this for a sparseMatrix of the class ~dgCMatrix~, the same technique of multiplying by a diagonalised matrix may be implemented, however to create this new matrix, a new ~sparseMatrix~ will need to be created using the properties of the original matrix, this can be done like so:


#+begin_src R :results output
 sparse_diag <- function(mat) {
  #' Diagonal Factors of Sparse Matrix
  #'
  #' Return a Diagonal Matrix of the 1 / colsum() such that
  #' matrix multiplication with this matrix would have all column sums
  #' sum to 1
  #'
  #' This should take the transpose of an adjacency matrix in and the output
  #' can be multiplied by the original matrix to scale it to 1.
  #' i

  ## Get the Dimensions
  n <- nrow(mat)

  ## Make a Diagonal Matrix of Column Sums
  D <- sparseMatrix(i = 1:n, j = 1:n, x = colSums(mat), dims = c(n,n))

  ## Throw away explicit Zeroes
  D <- drop0(D)

  ## Inverse the Values
  D@x <- 1/D@x

  ## Return the Diagonal Matrix
  return(D)
}
D <- sparse_diag(t(A))
summary(D)
#+end_src

#+RESULTS[96dcae690f8f87784cf7f17cd5d00136e2dee1be]:
#+begin_example
1000000 x 1000000 sparse Matrix of class "dgCMatrix", with 20 entries
        i      j           x
1   15447  15447 0.011235955
2   80297  80297 0.011764706
3  105277 105277 0.009174312
4  110381 110381 0.010638298
5  111018 111018 0.010416667
6  146681 146681 0.009708738
7  297550 297550 0.010309278
8  309389 309389 0.011111111
9  330573 330573 0.011764706
10 401507 401507 0.009615385
11 410107 410107 0.011363636
12 428268 428268 0.011363636
13 556574 556574 0.010638298
14 648515 648515 0.009708738
15 753918 753918 0.010752688
16 808361 808361 0.011111111
17 821733 821733 0.014084507
18 893744 893744 0.011363636
19 941850 941850 0.013698630
20 941903 941903 0.013698630
#+end_example

and hence the probability transition matrix may be implemented by performing matrix multiplication accordingly:

#+begin_src R :results output
summary(t(A) %*% D)
#+end_src

#+RESULTS[7f259bb86c55619e0d9d4ac000bb89535fa5473f]:
#+begin_example
1000000 x 1000000 sparse Matrix of class "dgCMatrix", with 20 entries
        i      j x
1  974487  15447 1
2  329285  80297 1
3  966175 105277 1
4  625266 110381 1
5  198129 111018 1
6  732490 146681 1
7  641236 297550 1
8  515236 309389 1
9  218343 330573 1
10 611744 401507 1
11 346309 410107 1
12  98950 428268 1
13 271674 556574 1
14 691630 648515 1
15 492423 753918 1
16  49547 808361 1
17 778479 821733 1
18 380886 893744 1
19 843950 941850 1
20 990749 941903 1
#+end_example

**** Solving the Random Surfer via the Power Method
Solving the eigenvalues for such a large matrix will not feasible, instead the power method will need to be used to find the stationary point.

However, creating a matrix of background probabilites (denoted by ~B~ is section [[#page-rank-random-surfer]]) will not be feasible, it would simply be too large, instead some algebra can be used to reduce $B$ from a matrix into a vector containing only $\frac{1-\alpha}{N}$.

The power method is given by:

\begin{align}
\vec{p}= \mathbf{S} \vec{p}
\end{align}

where:

\begin{align}
S &= \alpha \mathbf{T} +  \left( 1 - \alpha \right) \mathbf{B} \\
\vec{p} &= \left( \alpha \mathbf{T} +  \left( 1 - \alpha \right) \mathbf{B} \right) \vec{p}\\
&= \alpha \mathbf{T}\vec{p} +  \left( 1-\alpha \right) \mathbf{B} \vec{p}
\end{align}

Let $\mathbf{F}= \mathbf{B}\vec{p}$, consider the value of $\mathbf{F}$ :

\begin{align}
\mathbf{F} &=
\begin{bmatrix}
\frac{1}{N} & \frac{1}{N} & \ldots & \frac{1}{N} \\
\frac{1}{N} & \frac{1}{N} & \ldots & \frac{1}{N} \\
\vdots      & \vdots      & \ddots & \vdots \\
\frac{1}{N} & \frac{1}{N} & \ldots & \frac{1}{N} \\
\end{bmatrix} \label{eq:bgVal2}
\begin{bmatrix}
\vec{p_1} \\ \vec{p_2} \\ \vdots \\ \vec{p_m}
\end{bmatrix}  \\
&= \begin{bmatrix}
\left( \sum^{m}_{i= 0}   \left[ p_i \right]  \right) \times \frac{1}{N} \\
\left( \sum^{m}_{i= 0}   \left[ p_i \right]  \right) \times \frac{1}{N} \\
\vdots  \\
\left( \sum^{m}_{i= 0}   \left[ p_i \right]  \right) \times \frac{1}{N} \\
\end{bmatrix}  \\
& \text{Probabilities sum to 1 and hence:} \\
&= \begin{bmatrix}
\frac{1}{N} \\
\frac{1}{N} \\
\frac{1}{N} \\
\vdots  \\
\frac{1}{N} \\
\end{bmatrix}
\end{align}
So instead the power method can be implemented by performing an algorithm to the effect of:

#+begin_src R
## Find Stationary point of random surfer
N     <- nrow(A)
alpha <- 0.8
F     <- rep((1-alpha)/N, nrow(A))  ## A nx1 vector of (1-alpha)/N

## Solve using the power method
p     <- rep(0, length.out = ncol(T)); p[1] <- 1
p_new <- alpha*T %*% p + F

## use a Counter to debug
i <- 0
while (sum(round(p, 9) != round(p_new, 9))) {
    p     <- p_new
    p_new <- alpha*T %*% p + F
    (i <- i+1) %>% print()
}

p %>% head() %>% print()
#+end_src

#+RESULTS[ed72d96e5b69871f8a3e07792406adff2e6e1db2]:
: org_babel_R_eoe

** Power Walk Method
*** Introduction

\begin{align}
\mathbf{T} &= \mathbf{B} \mathbf{D}^{-1}_{B} \label{eq:pwalk-def}
\end{align}



where:

- $\mathbf{B}= \beta^{\mathbf{A}}$
  - $x\beta^{1}$  :: probability of following an edge of weight 1
  - $x\beta^{0}$  :: probability of following an edge of weight 0
  - $x\beta^{-1}$ :: probability of following an edge of weight -
- $D_{B} = \mathtt{colsums}(\mathbf{B})$
- $\mathbf{A}$ :: The Adjacency Matrix

*** Ordinary Matrices
Solving the Power walk can be done pretty much the same as it is with the random surfer, but doing it with Sparse Matrices is a bit trickier.
*** Sparse Matrices
**** Theory; Simplifying Power Walk to be solved with Sparse Matrices
The Random Surfer model is:

$$\begin{aligned}
    \mathbf{S} &= \alpha \mathbf{T} +  \mathbf{F}  \label{eq:sparse-RS}\end{aligned}$$

where:

- $\mathbf{T}$

  - is an $i \times j$ matrix that describes the probability of
    travelling from vertex $j$ to $i$

    - This is transpose from the way that =igraph= produces an adjacency
      matrix.

- $\mathbf{F} = \begin{bmatrix} \frac{1}{n} \\ \frac{1}{n} \\ \frac{1}{n} \vdots \end{bmatrix}$

Interpreting the transition probability matrix in this way is such that
$\mathbf{T}= \mathbf{A}\mathbf{D}^{- 1}_A$ under the following
conditions:


- No column of $\mathbf{A}$ sums to zero

  - If this does happen the question arises how to deal with
    $\mathbf{D_\mathbf{A}^{- 1}}$

    - I've been doing $\mathbf{D}^{\mathrm{T}}_{\mathbf{A}, i, j} := \mathtt{diag} \left( {\frac{1}{\mathtt{colsums}\left( \mathbf{A} \right)}} \right)$
      and then replacing any $0$ on the diagonal with 1.

  - What is done in the paper is to make another matrix $\mathbf{Z}$
    that is filled with 0, if a column sum of $\mathbf{A}$ adds to zero
    then that column in $\mathbf{Z}$ becomes $\frac{1}{n}$

    - This has the effect of making each row identical

    - The probability of going from an orphaned vertex to any other
      vertex would hence be $\frac{1}{n}$

    - The idea with this method is then to use
      $D_\mathbf{\left( A+Z \right)}^{- 1}$ this will be consistent with
      the /Random Surfer/ the method using $\mathbf{F}$ in
      [[#eq:sparse-RS][]] eqref:eq:sparse-RS

    where each row is identical that is a 0

The way to deal with the /Power Walk/ is more or less the same.

observe that:

$$\begin{aligned}
   \left( \mathbf{B} = \beta^{\mathbf{A}} \right)\wedge \left( \mathbf{A}_{i, j}\right)\in \mathbb{R}  \implies  \left\lvert \mathbf{B}_{i, j} \right\rvert > 0 \quad \forall i,j>n\in \mathbb{Z}^+ \label{eq:b-is-pos}\end{aligned}$$



Be mindful that the use of exponentiation in
[[#eq:b-is-pos][[eq:b-is-pos]]] is not an element wise exponentiation
and not an actual matrix exponential (which would be defined by using
power series and logs but is defined)

So if I have:

- $\mathbf{O}_{i, j} := 0, \quad \forall i,j\leq n \in \mathbb{Z}^+$

- $\vec{p_i}$ as the state distribution, being a vector of length $n$

Then It can be shown (see eqref:eq:sparse-power-walk):

$$\begin{aligned}
    \mathbf{O} \mathbf{D}_{\mathbf{B}}^{-1} \vec{p_i} = \mathtt{repeat} (\vec{p} \bullet \vec{\delta^{\tiny \mathrm{T}}} \mathtt{, n}\end{aligned})$$



where:

- $\vec{\delta_i} = \frac{1}{\mathtt{colsums} \left( \mathbf{B} \right)}$
  + A vector...($n\times 1$ matrix)
- $\vec{1}$  :: is a vector containing all 1's
  + A vector...($n\times 1$ matrix)
- $\vec{\delta^{\mathrm{T}}}$ :: refers to the transpoxe of $\vec{\detla}$ ($1\times n$ matrix)
- $\vec{\delta^{\mathrm{T}}} \vec{p_{i}}$ :: is some number (because it's a dot product)

This means we can do:

\begin{align}
  \overrightarrow{p_{i + 1}} & = \mathbf{T}_{\mathrm{pw}}
  \overrightarrow{p_i}\\
& = \mathbf{BD}_{\mathbf{B}}^{- 1}
  \overrightarrow{p_i}\\
  & = \left( \mathbf{B} - \mathbf{O} + \mathbf{O} \right)
  \mathbf{D}_{\mathbf{B}}^{- 1} \overrightarrow{p_i}\\
  & = \left( \left( \mathbf{B} - \mathbf{O} \right)
  \mathbf{D}_{\mathbf{B}}^{- 1} + \mathbf{OD}_{\mathbf{B}}^{- 1} \right)
  \overrightarrow{p_i}\\
  & = \left( \mathbf{B} - \mathbf{O} \right) \mathbf{D}_{\mathbf{B}}^{- 1}
  \overrightarrow{p_i} + \mathbf{OD}_{\mathbf{B}}^{- 1} \overrightarrow{p_i}\\
  & = \left( \mathbf{B} - \mathbf{O} \right) \mathbf{D}_{\mathbf{B}}^{- 1}
  \overrightarrow{p_i} + \vec{1} (\overrightarrow{\delta^{\mathrm{T}}}
  \overrightarrow{p_i}) \\
  & = \left( \mathbf{B} - \mathbf{O} \right) \mathbf{D}_{\mathbf{B}}^{- 1}
  \overrightarrow{p_i} + \mathtt{rep} (\overrightarrow{\delta^{\mathrm{T}}}
  \overrightarrow{p_i})
\end{align}

where:


Let $(\mathbf{B}-\mathbf{O}) = \mathbf{B_{\mathbf{O}}}$:

\begin{eqnarray*}
  \overrightarrow{p_{i + 1}} & = \mathbf{B_{\mathbf{O}}} \mathbf{D}_{\mathbf{B}}^{- 1}
  \overrightarrow{p_i} + \mathtt{rep} (\overrightarrow{\delta^{\mathrm{T}}}
  \overrightarrow{p_i}) &
\end{eqnarray*}

Now solve $\tmmathbf{D}_B^{- 1}$ in terms of $\mathbf{B_{O}}$ :

\begin{align}
  \mathbf{B}_{\mathbf{\mathbf{O}}} = & (\mathbf{B}-\mathbf{O})\\
  \mathbf{B} = & \mathbf{B}_{\mathbf{\mathbf{O}}}
  +\mathbf{O}
\end{align}

If we have $\delta_{\mathbf{B}}$ as the column sums of$\tmmathbf{\Beta}$ $\mathbf{B}$:

\begin{align}
\delta^{-1}_{\mathbf{B}} &= \vec{1}\mathbf{B} \\
&= \vec{1} \left( \mathbf{B_{O}} + \mathbf{O}\right) \\
&= \vec{1}  \mathbf{B_{O}} + \vec{1}\mathbf{O} \\
&= \vec{1} \mathbf{B_{\mathbf{O}}} + \langle n, n, n, ... n \rangle \\
&= \vec{1} \mathbf{B_{\mathbf{O}}} + \vec{1} n \\
\delta_{\mathbf{B}}&=\mathtt{1/(colSums(\mathbf{B_{O}}) + n )}
\end{align}

Then if we have $\mathit{{\tmstrong{{\tmem{D}}}}}_{\mathit{{\tmem{{\tmstrong{B}}}}}} =
\mathtt{diag} (\delta_{\tmmathbf{B}}) \mathtt{}$:


\[ \begin{array}{lll}
     \mathit{{\tmstrong{{\tmem{D}}}}}_{\mathit{{\tmem{{\tmstrong{B}}}}}}^{- 1}
     & = & \mathrm{diag} \left( \delta^{- 1}_{\mathbf{B}} \right)\\
     & = & \mathtt{diag} \left( \mathtt{ColSums}
     (\mathtt{\tmmathbf{B}_{\tmmathbf{O}}}) + \mathtt{n}
     \right)^{\mathtt{- 1}}
   \end{array} \]

And so the the power method can be implemented using sparse matrices:

\begin{align}
\vec{p_{i+1}} = \mathrm{B_{O}} \enspace \mathrm{diag}\left( \vec{1} \mathbf{B_{O}} + \vec{1}n \right) \vec{p_{i}} + \vec{1} \vec{\delta^{\mathrm{T}}\vec{p_{i}}}
\end{align}

in terms of */R/*:

#+begin_src R
p_new <- Bo %*% diag(colSums(B)+n) %*% p + rep(t(δ) %*% p, n)

# It would also be possible to sum the element-wise product
(t(δ) %*% p) == sum(δ * p)

# Because R treats vectors the same as a nX1 matrix we could also
# perform the dot product of the two vectors, meaning the following
# would be true in R but not generally

(t(δ) %*% p) == (δ %*% p)
#+end_src


***** Solving the Background Probability
In this case a vertical single column matrix will represent a vector and $\otimes$ will represent the outer product (i.e. the /Kronecker Product/):



   Define \(\vec{\delta}\) as the column sums of
\[\begin{aligned}
     \vec{\delta} & = \mathtt{colsum} (\text{{\bfseries{B}}})^{- 1}\\
     & = \frac{1}{\overrightarrow{1^{{\scriptsize \ensuremath{\boldsymbol{T}}}}}
     \ensuremath{\boldsymbol{B}}}
   \end{aligned}\]


Then we have:


\[ \begin{aligned}
     \mathbf{OD}_{\mathbf{B}}^{- 1} \overrightarrow{p_i} & = \left(
     \begin{array}{cccc}
       1 & 1 & 1 & \\
       1 & 1 & 1 & \ldots\\
       1 & 1 & 1 & \\
       & \vdots &  & \ddots
     \end{array} \right) \left( \begin{array}{cccc}
       \frac{1}{\delta_1} & 0 & 0 & \\
       0 & \frac{1}{\delta_2} & 0 & \ldots\\
       0 & 0 & \frac{1}{\delta_{13}} & \\
       & \vdots &  & \ddots
     \end{array} \right) \left( \begin{array}{c}
       p_{i, 1}\\
       p_{i, 2}\\
       p_{i, 3}\\
       \vdots
     \end{array} \right) \nonumber \nonumber\\
     & = \left( \begin{array}{cccccc}
       \frac{p_{i, 1}}{\delta 1} & + & \frac{p_{i, 2}}{\delta_2} & + &
       \frac{p_{i, 3}}{\delta_3} & \\
       \frac{p_{i, 1}}{\delta 1} & + & \frac{p_{i, 2}}{\delta_2} & + &
       \frac{p_{i, 3}}{\delta_3} & \ldots\\
       \frac{p_{i, 1}}{\delta 1} & + & \frac{p_{i, 2}}{\delta_2} & + &
       \frac{p_{i, 3}}{\delta_3} & \\
       &  & \vdots &  &  & \ddots
     \end{array} \right) \nonumber \nonumber\\
     & = \left( \begin{array}{c}
       \sum^n_{k = 1} [p_{i, k} \delta_i]\\
       \sum^n_{k = 1} [p_{i, k} \delta_i]\\
       \sum^n_{k = 1} [p_{i, k} \delta_i]\\
       \vdots
     \end{array} \right) \nonumber\\
     & = \left( \begin{array}{c}
       \overrightarrow{\delta^{{\footnotesize \tmmathbf{T}}}}
       \overrightarrow{p_i}\\
       \overrightarrow{\delta^{{\footnotesize \tmmathbf{T}}}} \vec{p}_i\\
       \overrightarrow{\delta^{{\footnotesize \tmmathbf{T}}}} \vec{p}_i\\
       \vdots
     \end{array} \right) \nonumber\\
     & = \overrightarrow{\delta^{{\footnotesize \tmmathbf{T}}}}
     \overrightarrow{p_i} \left( \begin{array}{c}
       1\\
       1\\
       1\\
       \vdots
     \end{array} \right) \nonumber\\
     & = (\overrightarrow{\delta^{{\footnotesize \tmmathbf{T}}}}
     \overrightarrow{p_i})  \vec{1}\\
     & = \mathtt{repeat} (\overrightarrow{\delta} \overrightarrow{p_i}
     \mathtt{, n}) \label{eq:sparse-power-walk}
   \end{aligned} \]
Observe also that If we let $\vec{\delta}$ and $p_i$ be 1 dimensional
vectors, this can also be expressed as a dot product:

   | Matrices                                | Vectors                    |
   | $\vec{\delta^{\mathrm{T}}} \vec{p_{i}}$ | $\vec{\delta} \vec{p_{i}}$ |

**** Practical; Implementing the Power Walk on Sparse Matrices
***** Inspect the newly created matrix and create constants
***** Setup
****** Load Packages
#+begin_src R :session example
if (require("pacman")) {
    library(pacman)
  }else{
    install.packages("pacman")
    library(pacman)
  }
  pacman::p_load(Matrix, igraph, plotly, mise, docstring, expm)
  mise()
#+end_src

#+RESULTS[b1fd4f7af2cadbc1374b0f8d78f62a6ad9342541]:
: Loading required package: pacman

****** Define function to create DiagonalsSparse Diagonal Function
This doesn't matter for the power walk, real exponents will always give non-zero values anyway
#+begin_src R :session example
sparse_diag <- function(mat) {
  #' Diagonal Factors of Sparse Matrix
  #'
  #' Return a Diagonal Matrix containing either 1 / colsum() or 0 such that
  #' matrix multiplication with this matrix would have all columns
  #' sum to 1
  #'
  #' This should take the transpose of an adjacency matrix in and the output
  #' can be multiplied by the original matrix to scale it to 1.
  #' i
  # mat  <- A
  ## Get the Dimensions
  n <- nrow(mat)

  ## Make a Diagonal Matrix of Column Sums
      ## If a column sums to zero the diag can be zero iff the adjacency_matrix>=0
  D <- sparseMatrix(i = 1:n, j = 1:n, x = colSums(mat), dims = c(n,n))

  ## Throw away explicit Zeroes
  D <- drop0(D)

  ## Inverse the Values
  D@x <- 1/D@x

  ## Return the Diagonal Matrix
  return(D)
}
#+end_src

#+RESULTS[b8327916d90bdc5810e057f7de6f3e0808ea7b88]:

****** Make an Example Graph
#+begin_src R :session example
g1 <- igraph::erdos.renyi.game(n = 20, 0.2)
A <- igraph::get.adjacency(g1) # Row to column


beta = 0.843234
β = beta
#+end_src
****** Plot

#+BEGIN_SRC R :exports both :session example :results output graphics file :file ./Media/Example-graph-plot-debug-power-walk.png
plot(g1)
#+END_SRC

#+RESULTS[5eec355fc3d55ed8cd2dd42e0f68ab07c9ca67fa]:
[[file:./Media/Example-graph-plot-debug-power-walk.png]]

***** Power Walk
****** Define B
#+begin_src R :session example
B      <- A
B@x    <- β^(A@x)
B      <- A
B       <- β^A

Bo     <- A

# These two approaches are equivalent
Bo@x   <- β^(A@x) -1   # This in theory would be faster
# Bo     <- β^(A) -1
# Bo     <- drop0(Bo)


  n <- nrow(A)
#+end_src

#+RESULTS[bc515375922834cfac37ab066bfcd2261fe752a0]:

#+begin_src R :session example :results output
print(B)
#+end_src

#+RESULTS[a32b596a74cff397c7bf190d87be4f0fa650f331]:
#+begin_example
20 x 20 Matrix of class "dgeMatrix"
          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]
 [1,] 1.000000 0.843234 1.000000 1.000000 1.000000 0.843234 1.000000 1.000000
 [2,] 0.843234 1.000000 1.000000 1.000000 0.843234 1.000000 1.000000 1.000000
 [3,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
 [4,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234
 [5,] 1.000000 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
 [6,] 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234
 [7,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
 [8,] 1.000000 1.000000 1.000000 0.843234 1.000000 0.843234 1.000000 1.000000
 [9,] 0.843234 1.000000 1.000000 1.000000 0.843234 1.000000 1.000000 1.000000
[10,] 0.843234 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
[11,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
[12,] 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234 1.000000 0.843234
[13,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234
[14,] 1.000000 0.843234 1.000000 0.843234 1.000000 0.843234 1.000000 1.000000
[15,] 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
[16,] 0.843234 1.000000 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000
[17,] 1.000000 1.000000 0.843234 0.843234 1.000000 1.000000 0.843234 0.843234
[18,] 1.000000 1.000000 1.000000 1.000000 0.843234 1.000000 1.000000 1.000000
[19,] 1.000000 0.843234 0.843234 1.000000 1.000000 1.000000 1.000000 0.843234
[20,] 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]
 [1,] 0.843234 0.843234 1.000000 1.000000 1.000000 1.000000 0.843234 0.843234
 [2,] 1.000000 0.843234 1.000000 1.000000 1.000000 0.843234 1.000000 1.000000
 [3,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234
 [4,] 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234 1.000000 1.000000
 [5,] 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
 [6,] 1.000000 1.000000 1.000000 0.843234 1.000000 0.843234 1.000000 1.000000
 [7,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
 [8,] 1.000000 1.000000 1.000000 0.843234 0.843234 1.000000 1.000000 1.000000
 [9,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
[10,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234 1.000000
[11,] 1.000000 1.000000 1.000000 1.000000 0.843234 1.000000 0.843234 1.000000
[12,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234
[13,] 1.000000 1.000000 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000
[14,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234
[15,] 1.000000 0.843234 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000
[16,] 1.000000 1.000000 1.000000 0.843234 1.000000 0.843234 1.000000 1.000000
[17,] 1.000000 1.000000 0.843234 0.843234 0.843234 1.000000 1.000000 1.000000
[18,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
[19,] 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234 1.000000 1.000000
[20,] 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234 1.000000 0.843234
         [,17]    [,18]    [,19]    [,20]
 [1,] 1.000000 1.000000 1.000000 0.843234
 [2,] 1.000000 1.000000 0.843234 1.000000
 [3,] 0.843234 1.000000 0.843234 1.000000
 [4,] 0.843234 1.000000 1.000000 1.000000
 [5,] 1.000000 0.843234 1.000000 1.000000
 [6,] 1.000000 1.000000 1.000000 1.000000
 [7,] 0.843234 1.000000 1.000000 1.000000
 [8,] 0.843234 1.000000 0.843234 1.000000
 [9,] 1.000000 1.000000 1.000000 1.000000
[10,] 1.000000 1.000000 1.000000 1.000000
[11,] 0.843234 1.000000 1.000000 1.000000
[12,] 0.843234 1.000000 1.000000 1.000000
[13,] 0.843234 1.000000 1.000000 1.000000
[14,] 1.000000 1.000000 0.843234 0.843234
[15,] 1.000000 1.000000 1.000000 1.000000
[16,] 1.000000 1.000000 1.000000 0.843234
[17,] 1.000000 0.843234 0.843234 1.000000
[18,] 0.843234 1.000000 0.843234 1.000000
[19,] 0.843234 0.843234 1.000000 1.000000
[20,] 1.000000 1.000000 1.000000 1.000000
#+end_example


#+begin_src R :session example :results output
print(Bo)
#+end_src

#+RESULTS[bad3b48f1882200a41147309b6b523c317c6e071]:
#+begin_example
20 x 20 sparse Matrix of class "dgCMatrix"

 [1,]  .        -0.156766  .         .         .        -0.156766  .
 [2,] -0.156766  .         .         .        -0.156766  .         .
 [3,]  .         .         .         .         .         .         .
 [4,]  .         .         .         .         .         .         .
 [5,]  .        -0.156766  .         .         .         .         .
 [6,] -0.156766  .         .         .         .         .         .
 [7,]  .         .         .         .         .         .         .
 [8,]  .         .         .        -0.156766  .        -0.156766  .
 [9,] -0.156766  .         .         .        -0.156766  .         .
[10,] -0.156766 -0.156766  .         .         .         .         .
[11,]  .         .         .         .         .         .         .
[12,]  .         .         .         .         .        -0.156766  .
[13,]  .         .         .         .         .         .         .
[14,]  .        -0.156766  .        -0.156766  .        -0.156766  .
[15,] -0.156766  .         .         .         .         .         .
[16,] -0.156766  .        -0.156766  .         .         .         .
[17,]  .         .        -0.156766 -0.156766  .         .        -0.156766
[18,]  .         .         .         .        -0.156766  .         .
[19,]  .        -0.156766 -0.156766  .         .         .         .
[20,] -0.156766  .         .         .         .         .         .

 [1,]  .        -0.156766 -0.156766  .         .         .         .
 [2,]  .         .        -0.156766  .         .         .        -0.156766
 [3,]  .         .         .         .         .         .         .
 [4,] -0.156766  .         .         .         .         .        -0.156766
 [5,]  .        -0.156766  .         .         .         .         .
 [6,] -0.156766  .         .         .        -0.156766  .        -0.156766
 [7,]  .         .         .         .         .         .         .
 [8,]  .         .         .         .        -0.156766 -0.156766  .
 [9,]  .         .         .         .         .         .         .
[10,]  .         .         .         .         .         .         .
[11,]  .         .         .         .         .        -0.156766  .
[12,] -0.156766  .         .         .         .         .         .
[13,] -0.156766  .         .        -0.156766  .         .         .
[14,]  .         .         .         .         .         .         .
[15,]  .         .        -0.156766 -0.156766  .         .         .
[16,]  .         .         .         .        -0.156766  .        -0.156766
[17,] -0.156766  .         .        -0.156766 -0.156766 -0.156766  .
[18,]  .         .         .         .         .         .         .
[19,] -0.156766  .         .         .         .         .        -0.156766
[20,]  .         .         .         .         .         .        -0.156766

 [1,] -0.156766 -0.156766  .         .         .        -0.156766
 [2,]  .         .         .         .        -0.156766  .
 [3,]  .        -0.156766 -0.156766  .        -0.156766  .
 [4,]  .         .        -0.156766  .         .         .
 [5,]  .         .         .        -0.156766  .         .
 [6,]  .         .         .         .         .         .
 [7,]  .         .        -0.156766  .         .         .
 [8,]  .         .        -0.156766  .        -0.156766  .
 [9,]  .         .         .         .         .         .
[10,] -0.156766  .         .         .         .         .
[11,] -0.156766  .        -0.156766  .         .         .
[12,]  .        -0.156766 -0.156766  .         .         .
[13,]  .         .        -0.156766  .         .         .
[14,]  .        -0.156766  .         .        -0.156766 -0.156766
[15,]  .         .         .         .         .         .
[16,]  .         .         .         .         .        -0.156766
[17,]  .         .         .        -0.156766 -0.156766  .
[18,]  .         .        -0.156766  .        -0.156766  .
[19,]  .         .        -0.156766 -0.156766  .         .
[20,]  .        -0.156766  .         .         .         .
#+end_example

****** Solve the Scaling Matrix
We don't need to worry about any terms of $\delta_{\mathbf{B}} = \mathtt{colsums\left(B\_o\right)+n}$ being 0:

#+begin_src R :session example
(δB   <- 1/(colSums(Bo)+n))
#+end_src

#+RESULTS[4e9078e02cac88e2b390d1de64aa94cfe1ea0faa]:
:  [1] 0.05290267 0.05203951 0.05120406 0.05120406 0.05120406 0.05161840
:  [7] 0.05039501 0.05246754 0.05079631 0.05120406 0.05120406 0.05161840
: [13] 0.05120406 0.05246754 0.05120406 0.05203951 0.05379495 0.05120406
: [19] 0.05246754 0.05120406

#+begin_src R :session example
(δB   <- 1/(colSums(B)))
#+end_src

#+RESULTS[dad08f4612601febd1e915b158e85aad0430943b]:
:  [1] 0.05290267 0.05203951 0.05120406 0.05120406 0.05120406 0.05161840
:  [7] 0.05039501 0.05246754 0.05079631 0.05120406 0.05120406 0.05161840
: [13] 0.05120406 0.05246754 0.05120406 0.05203951 0.05379495 0.05120406
: [19] 0.05246754 0.05120406


****** Find the Transition Probability Matrix
#+begin_src R :session example
  DB   <- diag(δB)
## ** Create the Transition Probability Matrix
## Create the Trans Prob Mat using Power Walk
  T <- Bo %*% DB
#+END_SRC

****** Implement the Loop
#+begin_src R :session example
## ** Implement the Power Walk
## *** Set Initial Values
  p_new  <- rep(1/n, n)  # Uniform
  p      <- rep(0, n)    # Zero
  η      <- 10^(-6)
## *** Implement the Loop

 while (sum(abs(p_new - p)) > η) {
    (p <- as.vector(p_new)) # P should remain a vector
    sum(p <- as.vector(p_new)) # P should remain a vector
     p_new  <- T %*% p + rep(t(δB) %*% p, n)
  }
## ** Report the Values
print(paste("The stationary point is"))
print(p)
#+end_src

#+RESULTS[95b35a53d822393ea1522a4e5bc714a0cd0834e2]:
: [1] "The stationary point is"
:  [1] 0.04882572 0.04963556 0.05044542 0.05044541 0.05044543 0.05004049
:  [7] 0.05125527 0.04923064 0.05085035 0.05044543 0.05044542 0.05004049
: [13] 0.05044542 0.04923064 0.05044543 0.04963557 0.04801586 0.05044542
: [19] 0.04923063 0.05044542

** Creating a Package
* Relating the Power Walk to the Random Surfer
  :PROPERTIES:
  :CUSTOM_ID: relating-terms-in-power-walk-to-random-surfer
  :END:
** Introduction
These are notes relating to [[cite:parkPowerWalkRevisiting2013][\textsection 3.3]]

So if a term in the Power Walk can be related to $\alpha$ in the random
surfer, which is in turn $\xi_2$, I'll be able to understand it better. [fn:: Although I'm not quite sure why $\alpha$ is $\xi_{2}$ either]

Consider the equation:


\begin{align*}
\mathbf{T}&= \mathbf{B}\mathbf{D}_{\mathbf{B}}^{- 1} \\
&= \left( \mathbf{B}+  \mathbf{O} - \mathbf{O} \right) \mathbf{D}_{\mathbf{B}}^{- 1} \\
\end{align*}


Break this into to terms so that we can simplify it a bit:


\begin{align*}
    \mathbf{T} &= \Bigg[ \left( \mathbf{B}- \mathbf{O} \right)\mathbf{D}_{\mathbf{B}}^{- 1} \Bigg] + \Bigg\{  \mathbf{O}\mathbf{D}_{\mathbf{B}}^{- 1} \Bigg\}
\end{align*}
** Value of [1st Term]
   :PROPERTIES:
   :CUSTOM_ID: value-of-1st-term
   :END:

Observe that for all $\forall i,j\in \mathbb{Z}^+$:


\begin{align*}
\mathbf{A}_{i, j} \in \left\{0, 1\right\} \\
\implies  \mathbf{B}^{\mathbf{A}_{i, j}} &\in \left\{\beta^0, \beta^1\right\} \\
                     &= \left\{1, \beta \right\}  \\
                      \implies  \beta \mathbf{A} = \left\{1, \beta \right\}
\end{align*}


Using this property we get the following


\begin{align*}
\mathbf{B}_{i,j}- \mathbf{O}_{i,j} = \left( \beta^{\mathbf{A}_{i,j}} -1 \right) &=
\begin{cases}
    0      , &\enspace \mathbf{A}_{i,j}=0  \\
    \beta-1, &\enspace \mathbf{A}_{i,j}=1  \\
\end{cases} \\
\left( \beta- 1 \right) \mathbf{A}_{i,j} &=
\begin{cases}
    0      , &\enspace \mathbf{A}_{i,j}=0  \\
    \beta-1, &\enspace \mathbf{A}_{i,j}=1  \\
\end{cases} \\
\end{align*}


This means we have


\begin{align*}
\mathbf{A} \in \left\{0, 1\right\} \forall i,j  \implies   \mathbf{B}_{i,j}- \mathbf{O}_{i,j} &= \left( \beta-1 \right) \mathbf{A}_{i,j}
\end{align*}



\begin{align*}
\mathbf{B}&= \left( \mathbf{B}+  \mathbf{O}- \mathbf{O} \right) \\
&= \left( \mathbf{B}- 1 \right)
\end{align*}

** Value of {2nd Term}
  :PROPERTIES:
  :CUSTOM_ID: value-of-2nd-term
  :END:


\begin{align*}
\mathbf{O} \mathbf{D_B^{- 1}} &=
\begin{pmatrix}
    1 & 1      & 1 &        \\
    1 & 1      & 1 &\cdots  \\
    1 & 1      & 1 &        \\
      & \vdots &   &\ddots
\end{pmatrix}
\begin{pmatrix}
    \frac{1}{\delta_1} & 1                    & 1                   & \\
    1                  & \frac{1}{\delta_{2}} & 1 \cdots            & \\
    1                  & 1                    &  \frac{1}{\delta_3} & \\
               & \vdots &             &                     \ddots
\end{pmatrix}
\\
&= n
\begin{pmatrix}
    \frac{1}{n} & \frac{1}{n}      & \frac{1}{n} &        \\
    \frac{1}{n} & \frac{1}{n}      & \frac{1}{n} &\cdots  \\
    \frac{1}{n} & \frac{1}{n}      & \frac{1}{n} &        \\
      & \vdots &   &\ddots
\end{pmatrix}
\begin{pmatrix}
    \frac{1}{\delta_1} & 1                    & 1                   &        \\
    1                  & \frac{1}{\delta_2}    & 1                   & \cdots \\
    1                  & 1                    &  \frac{1}{\delta_3} &        \\
                       & \vdots               &                     & \ddots
\end{pmatrix}
\\
&= n \mathbf{E}\mathbf{D_B}^{-1}
\end{align*}


where the following definitions hold ($\forall i, j \in \mathbb{Z}^+$):

- $\mathbf{E}_{i, j} = \frac{1}{n}$
- $\mathbf{D_B}^{-1}_{k, k} = \frac{1}{\delta_k}$
- The value of $\delta$ is value that each term in a column must be
  divided by to become zero, in the case of the power walk that is just
  $\frac{1}{\mathtt{colSums}\left( \mathbf{B} \right)} = \vec{1}\mathbf{B}$,
  but if there were zeros in a column, it would be necessary to swap out
  the $0$s for $1$s and then sum in order to prevent a division by zero
  issue and because the 0s should be left.
- $\mathbf{A}\in \left\{0, 1\right\} \forall i,j$ is the unweighted
  adjacency matrix of the relevant graph.

putting this all together we can do the following:


\begin{align*}
\mathbf{T}&= \mathbf{B}\mathbf{D}^{- 1}_{\mathbf{B}} \\
&= \left( \mathbf{B}+  \mathbf{O} - \mathbf{O} \right) \mathbf{D}_{\mathbf{B}}^{- 1} \\
&= \left( \mathbf{B}- \mathbf{O} \right)\mathbf{D}_{B}^{- 1}  +  \mathbf{O} {\mathbf{D}_{\mathbf{B}}^{- 1}} \\
 \intertext{From above:} \\
&= \left( \beta- 1 \right) \mathbf{A}_{i,j} +  n \mathbf{E} \mathbf{D}_{\mathbf{B}}^{- 1}\\
&= \mathbf{A}_{i,j}\left( \beta- 1 \right)  +  n \mathbf{E} \mathbf{D}_{\mathbf{B}}^{- 1}\\
 \intertext{because $\mathbf{D} \mathbf{D}^{- 1} = \mathbf{I}$ we can multiply one side through:} \\
&= \mathbf{D}_{\mathbf{A}} \mathbf{D}_{\mathbf{A}}^{- 1}\mathbf{A}_{i,j}\left( \beta- 1 \right)  +  n \mathbf{E} \mathbf{D}_{\mathbf{B}}^{- 1}\\
\end{align*}


But the next step requires showing that:


\begin{align*}
\left( \beta-1 \right)\mathbf{D}_\mathbf{A} \mathbf{D}_{\mathbf{B}}^{- 1} &= \mathbf{I} - n \mathbf{D}_{B}^{- 1}
\end{align*}

** Equate the Power Walk to the Random Surfer
Define the matrix $\mathbf{D}_{\mathbf{M}}$:

\begin{align}
    \mathbf{D}_{\mathbf{M}} = \mathrm{diag}\left( \mathtt{colSum} \left( \mathbf{M} \right) \right) &= \mathrm{diag} \left( \vec{1} \mathbf{M} \right)
\end{align}


To scale each column of that matrix to 1, each column will need to be divieded by the column sum, unless the column is already zero, this needs to be done to turn an adjacency matrix into a matrix of probabilities:

\begin{align}
    \mathbf{D}_{\mathbf{A}} ^{- 1} :  \left[     \mathbf{D}_{\mathbf{A}} ^{- 1}  \right]_i =
    \begin{cases}
	0 ,& \quad \left[ \mathbf{D}_{\mathbf{A}} \right]_i = 0 \\
	\left[ \frac{1}{\mathbf{D}_{\mathbf{A}}} \right] ,& \enspace \enspace \left[ \mathbf{D}_{\mathbf{A}} \right]_i \neq 0
    \end{cases}
\end{align}

In the case of the power walk $\mathbf{B}= \beta^{\mathbf{A}} \neq 0$ so it is sufficient:

\begin{align}
    \mathbf{D}_{\mathbf{B}}^{- 1} &= \frac{1}{\mathrm{diag}\left( \vec{1} \left(\mathbf{\beta^{\mathbf{A}}  \right) } \right)}
\end{align}


Recall that the /power walk/ gives a transition probability matrix:

\begin{align}
%    \mathbf{T} &= \mathbf{a} \text{\fboxsep=.2em\fbox{$x$}} \\
    \text{\textbf{Power Walk}} \nonumber \\
\mathbf{T} &= \text{\fboxsep=.2em\fbox{$\mathbf{A}\mathbf{D}_{\mathbf{A}}^{- 1}$}}  \mathbf{D}_{\mathbf{A}} \left( \beta - 1 \right) \mathbf{D}_{\mathbf{B}}^{- 1} + \text{\fboxsep=.2em\fbox{$\mathbf{E}$}} n \mathbf{D}_{\mathbf{B}}^{- 1}  \label{eq:pwbx}\\
    \text{\textbf{Random Surfer}} \nonumber \\
    \mathbf{T} &= \alpha \text{\fboxsep=.2em\fbox{$\mathbf{A}\mathbf{D}_{\mathbf{A}}^{- 1}$}}  + \left( 1-\alpha \right) \text{\fboxsep=.2em\fbox{$\mathbf{E}$}}
\end{align}

So these are equivalent when:

\begin{align}
\mathbf{D}_{\mathbf{A}}   \left( \beta -  1 \right)\mathbf{D}_{\mathbf{B}^{- 1}} &=\mathbf{I}  \alpha \label{fl} \\
    \ \nonumber \\
  \vec{1}  \left( 1- \alpha \right) &=  - n \mathbf{D}_{\mathbf{B}}^{- 1}  \nonumber \\
    \implies  \vec{1}\alpha &=  \vec{1}- n \mathbf{D}_{\mathbf{B}}^{- 1} \label{st} \\
    \intertext{Hence we have:} \notag \\
\mathbf{D}_{\mathbf{A}}  \left( \beta -  1 \right)\mathbf{D}_{\mathbf{B}}^{- 1} &=  \vec{1}\alpha =  \mathbf{I}- n \mathbf{D}_{\mathbf{B}}^{- 1} \label{eq:eqalpha}
\end{align}


Solving for $\beta$  with eqref:fl :

\begin{align}
    \beta&= \frac{1- \Theta}{\Theta}\\
%    \beta&= \frac{\alpha - \mathbf{D}_{\mathbf{A}}\mathbf{D}_{\mathbf{B}}^{- 1}}{\mathbf{D}_{\mathbf{A}}\mathbf{D}_{\mathbf{B}}^{-1}}
\end{align}

where: [fn:bvl]

- $\Theta = \mathbf{D}_{\mathbf{A}} \mathbf{D}_{\mathbf{B}}^{- 1}$

but we can't really do this so instead:

\[
\beta \mathbf{1}_{\tiny \left[ n,n \right]}  = \left( 1 - \Theta \right) \Theta^{-1} \label{eq:betadef}
\]

If $\beta$ is set accordingly then by eqref:eq:eqalpha:

\begin{align}
    \mathbf{A}\left( \beta- 1 \right) \mathbf{D}_{\mathbf{B}}^{- 1} &= \alpha = \mathbf{I}- n \mathbf{D}_{\mathbf{B}}^{- 1} \nonumber \\
     \implies  \mathbf{A}\left( \beta- 1 \right) \mathbf{D}_{\mathbf{B}}^{- 1} &=  \mathbf{I}- n \mathbf{D}_{\mathbf{B}}^{- 1}
\end{align}

And setting $\Gamma = \mathbf{I}- n \mathbf{D}_{\mathbf{B}}^{- 1}$  from eqref:st and putting in \eqref{eq:pwbx} we have:

\begin{align}
\mathbf{T} &= \text{\fboxsep=.2em\fbox{$\mathbf{A}\mathbf{D}_{\mathbf{A}}^{- 1}$}}  \mathbf{D}_{\mathbf{A}} \left( \beta - 1 \right) \mathbf{D}_{\mathbf{B}}^{- 1} + \text{\fboxsep=.2em\fbox{$\mathbf{E}$}} n \mathbf{D}_{\mathbf{B}}^{- 1}  \nonumber \\
  \mathbf{T} &= \Gamma \text{\fboxsep=.2em\fbox{$\mathbf{A}\mathbf{D}_{\mathbf{A}}^{- 1}$}}  + \left( 1-\Gamma \right) \text{\fboxsep=.2em\fbox{$\mathbf{E}$}} \nonumber \\
  \ \nonumber \\
  \mathbf{T} &= \Gamma \mathbf{A}\mathbf{D}_{\mathbf{A}}^{- 1}  + \left( 1-\Gamma \right) \mathbf{E}
  \end{align}

  Where $\mathbf{E}$ is square matrix of $\frac{1}{n}$ as in eqref:eq:bgval1  eqref:eq:bgVal2

** Conclusion
So when the adjacency matrix is stictly boolean, the power walk is equivalent to the random surfer.

** TODO The Second Eigenvalue
*** TODO The Random Surfer
The Second eigenvalue \(\xi_2\) of the Power Surfer is less than $\alpha$ ([[file:Proposal/Propsal.org::#stability-convergence][See 3.2; Stability and Concvergence, of proposal]]).
*** TODO Power Walk
Because the Power Walk relates to the random surfer as demonstrated in section [[#relating-terms-in-power-walk-to-random-surfer]], what can be said about $\xi_{2}$
**** Applying this to Power Walk
Let $\Lambda_{\left( 2 \right)}\left( \mathbf{T} \right) = \lambda_2$ return the second value of a transition, probability Matrix, then observe that:


\begin{align}
    \Lambda_{\left( 2 \right)} \left( \mathbf{T}_{\text{\tiny RS}} \right)  \leq \left\lvert \alpha \right\rvert  \implies      \Lambda_{\left( 2 \right)} \left( \mathbf{T}_{\text{\tiny PW}} \right) \leq \left\lvert \frac{\alpha - \mathbf{D}_{\mathbf{a}} \mathbf{D}_{\mathbf{B}}^{- 1}}{\mathbf{D}_{\mathbf{A}}\mathbf{D}_{\mathbf{B}}^{-1}}  \right\rvert
\end{align}

where:


 - $\lambda_{\left( 2 \right)} \left( \mathbf{T} \right)$ refers to the transition probability matrix of the power walk and random surfer approaces as indicated.
***** My attempt
\begin{align}
    \beta \mathbf{1}_{\tiny \left[ n, n \right] }    &= \frac{1- \Theta}{\Theta} \label{eq:betasig}\\
%    \beta&= \frac{\alpha - \mathbf{D}_{\mathbf{A}}\mathbf{D}_{\mathbf{B}}^{- 1}}{\mathbf{D}_{\mathbf{A}}\mathbf{D}_{\mathbf{B}}^{-1}}
\end{align}

where:
- $\Theta = \mathbf{D}_{\mathbf{A}} \mathbf{D}_{\mathbf{B}}^{- 1}$

So I thought maybe if I could find a value of $\beta$ that satisfied eqref:eq:betasig then I could show circumstances under which $\left\lvert \xi_2 \right\rvert < \alpha$.

Seemingly it's only satisfied where $\beta = 1$ though, using this simulation:

#+begin_src R
g1 <- igraph::erdos.renyi.game(n = 9, 0.2)
A <- igraph::get.adjacency(g1) # Row to column
A <- t(A)
# plot(g1)

## * Finding beta values to behave like Random Surfer
  beta <- 10
  B <- beta^A

  DA     <- PageRank::create_sparse_diag_sc_inv_mat(A)
  DB_inv <- PageRank::create_sparse_diag_scaling_mat(B)

 THETA <- DA %*% DB_inv

THETA <- function(A, beta) {
  B  <- beta^A
  DA     <- PageRank::create_sparse_diag_sc_inv_mat(A)
  DB_inv <- PageRank::create_sparse_diag_scaling_mat(B)
  return(DA %*% DB_inv)
}

THETA_inv <- function(A, beta) {
  B  <- beta^A
  DB     <- PageRank::create_sparse_diag_sc_inv_mat(B)
  DA_inv <- PageRank::create_sparse_diag_scaling_mat(A)
  return(DA %*% DB_inv)
}

beta_func <- function(A, beta) {
    return(1-THETA(A, beta^A) %*% THETA_inv(A, beta^A))
}

THETA(A, 10) %*% THETA_inv(A, 10)


eta <- 10^-6
beta <- 1.01
while (mean(beta*matrix(1, nrow(A), ncol(A)) - beta_func(A, beta)) > eta) {
    beta <- beta + 0.01
    print(beta)
    print(diag(beta_func(A, beta)))
    print(beta*matrix(1, nrow(A), ncol(A)))
    print(beta_func(A, beta))
#    Sys.sleep(0.1)
}

beta


diag(beta_func(A, beta))
beta


## * blah
#+end_src
* Simulating the Structure of the Web
A graph of the internet is /scale free/, this means that the number of nodes of a graph (\(n\)), having \(j\) edges is given by [[cite:langvilleGooglePageRankScience2012][\textsection 10.7.2]]:

\begin{align}
n \propto j^{-k}, \quad \exists k \in \mathbb{R}
\end{align}

The /Erdos Renyi/ game is a random network, a superior approach to model the web is to use a scale free networks cite:barabasiPhysicsWeb2001 such as the Barabasi-Albert graph cite:barabasiScalefreeCharacteristicsRandom2000

* Investigating the Second EigenValue

Maybe I should look at the most appropriate way to simulate social network links, one possibility is [[https://crpit.scem.westernsydney.edu.au/confpapers/CRPITV144Zeng.pdf][this paper ]] cite:zengPracticalSimulationMethod2013.

Actually there is a data set available
 cite:garritanoWikipediaArticleNetworks2019, I should just analyse that, see [[file:~/Dropbox/DataSci/Visual_Analytics/Assessment/the-marvel-universe-social-network/plotly3d_Marvel.r][how
it was done in Visual Analytics as a reminder]].

Using the Wikipedia ArtYeah I think that's right, thaicle compare density and Determinant.

Is the determinant easily calculated for a large matrix?
  It appears to diverge

  Will the determinant diverge for large matrices?
  Will the prob of making edges in the game just be the density?

  Look at comparing the determinant and the density of the wikipedia adjacency matrix.

  What are some ways that we can model the second eigenvalue?

** Plotting Various Values

There is some relationship between the determinant and the density, check out the pairs plot:



#+BEGIN_SRC R :exports both :results output graphics file :file Media/EigenValue_Determinant.png :eval never-export
  library(pacman)
  pacman::p_load(PageRank, devtools, Matrix, igraph, tidyverse)
n <- 20
p <- 1:n/n
beta <- 1:n/n
beta <- runif(n)*100
sz <- 1:n/n+10
input_var <- expand.grid("n" = n, "p" = p, "beta" = beta, "size" = sz)
input_var


random_graph <- function(n, p, beta, size) {
      g1 <- igraph::erdos.renyi.game(n = sz, p)
      A <- igraph::get.adjacency(g1) # Row to column
      A <- Matrix::t(A)

      A_dens <- mean(A)
      T      <- PageRank::power_walk_prob_trans(A)
      e2     <- eigen(T, only.values = TRUE)$values[2] # R orders by descending magnitude
      A_det  <- det(A)
      return(c(abs(e2), A_det))
}

## TODO this should use pmap.
Y <- matrix(ncol = 2, nrow = nrow(input_var))
for (i in 1:nrow(input_var)) {
  X <- as.vector(input_var[i,])
  Y[i,] <-  random_graph(X$n, X$p, X$beta, X$size)
}
if (sum(abs(Y) != abs(Re(Y))) == 0) {
  Y <- Re(Y)
}
nrow(input_var)
nrow(Y)
Y <- as.data.frame(Y); colnames(Y) <- c("eigenvalue2", "determinant")

data <- cbind(input_var, Y)

ggplot(data, aes(x = determinant, y = eigenvalue2, size = beta, color = size, shape = factor(n))) +
  geom_point() +
  labs(x = "Determinant of Adjacency Matrix", y = "Second Eigenvalue of Power Walk Transition Probability Matrix") +
  scale_size_continuous(range = c(0.1,1))
#+end_src

#+RESULTS[08882d661cf5d2410c8335d1850632709c7cf5c5]:
[[file:../Media/EigenValue_Determinant.png]]


#+BEGIN_SRC R :exports both :results output graphics file :file Media/EigenValue_Density.png :eval never-export
  library(pacman)
  pacman::p_load(PageRank, devtools, Matrix, igraph, tidyverse)
n <- 100
p <- 1:n/n
beta <- 1:n/n
beta <- runif(n)*100
sz <- 1:n/n+10
input_var <- expand.grid("n" = n, "p" = p, "beta" = beta, "size" = sz)
input_var


random_graph <- function(n, p, beta, size) {
      g1 <- igraph::erdos.renyi.game(n = sz, p)
      A <- igraph::get.adjacency(g1) # Row to column
      A <- Matrix::t(A)

      A_dens <- mean(A)
      T      <- PageRank::power_walk_prob_trans(A)
      e2     <- eigen(T, only.values = TRUE)$values[2] # R orders by descending magnitude
      A_det  <- det(A)
      return(c(abs(e2), A_dens))
}

## TODO this should use pmap.
Y <- matrix(ncol = 2, nrow = nrow(input_var))
for (i in 1:nrow(input_var)) {
  X <- as.vector(input_var[i,])
  Y[i,] <-  random_graph(X$n, X$p, X$beta, X$size)
}
if (sum(abs(Y) != abs(Re(Y))) == 0) {
  Y <- Re(Y)
}
nrow(input_var)
nrow(Y)
Y <- as.data.frame(Y); colnames(Y) <- c("eigenvalue2", "determinant")

data <- cbind(input_var, Y)

ggplot(data, aes(x = determinant, y = eigenvalue2, color = size, shape = factor(n))) +
  geom_point(base_size = 99, aes(size = beta)) +
  labs(x = "Density of Adjacency Matrix", y = "Second Eigenvalue of Power Walk Transition Probability Matrix") +
  scale_size_continuous(range = c(0.1,1))
#+end_src

Maybe this looks like a Chi distribution?

#+BEGIN_SRC R :exports both :results output graphics file :file Media/EigenValue_Density_Chi.png :eval never-export
chival <- dchisq(seq(from = 0, to = 40, length.out = 100), df = 10)*6
index  <- seq(from = 0, to = 2, length.out = 100)
chidata  <- data.frame(index = index, chi = chival)
ggplot(data) +
  geom_point(mapping = aes(x = determinant, y = eigenvalue2, size = beta, color = size, shape = factor(n))) +
  geom_line(data = chidata, mapping = aes(x = index, y = chi)) +
  scale_size_continuous(range = c(0.1,1)) +
  labs(x = "Density of Adjacency Matrix", y = "Second Eigenvalue of Power Walk Transition Probability Matrix")
#+end_src

#+RESULTS[c7a830cfab9be72b1ce3782f148a5dcb92c49f48]:
[[file:../Media/EigenValue_Density_Chi.png]]

** TODO Model the log transformed data using a linear regression or log(-x) regression

\begin{align}
    \xi_2 &= \left( 1-  \frac{\sum^{n}_{i= 1} \sum^{n}_{j= 1}   \mathbf{A}_{i,j}  }{n^{2}} \right)^{0.6} \cdot  e^{- 0.48} \pm \Delta
\end{align}

*** TODO Change the colour of each model by using pivot_longer
** TODO Could I get better performance by also considering the determinant?
No not really, it terms of accuracy

** TODO Is the determinant faster or slower?
Significantly slower for large matrices.
** TODO Import wikipedia data
- +Import the wikipedia data+
- +Measure the density+
- +Use the density to guess the \(p\) of the game+
  + +Justify the witht the scatterplot matrix+
- +Measure the affect of different \(\beta\) values on \(\lambda_2\) for graphs ov various sizes given that \(p\) value.+
  + +Or atleast a range within that prob+

    use a /Barabassi-Albert/ Random Graph through the ~igraph::

* Cauchy Integral Formula
  :PROPERTIES:
  :CUSTOM_ID: cauchy-integral-formula
  :END:

This is from section 54 of the book, isn't it nice that it more or less
just works hey? cite:zhangMakingEigenvectorBasedReputation2004

\begin{align}
f\left( a \right) \frac{1}{2\pi i} \oint \frac{f\left( z \right)}{z- a}\mathrm{d}z
\end{align}

In view of this equation then: cite:zhangMakingEigenvectorBasedReputation2004

$$\begin{aligned}
\left| \int_C \frac{f\left( z \right)}{z- z_0} \mathrm{d}z - 2 \pi i f\left( z_0 \right) \right|<2 \pi \varepsilon
\end{aligned}$$

Some Images: cite:ngStableAlgorithmsLink2001

#+NAME: testim
#+CAPTION: This image is for testing purposes cite:moskowitzLibraryGuidesWikipedia
#+attr_html: :width 400px
#+attr_latex: :width 12cm
[[file:media/my-self-rep-frac.png]]

#+NAME: testtikzins
#+CAPTION: This is a tikz image inserted as a png from imagemagick
#+attr_html: :width 400px
#+attr_latex: :width 12cm
[[file:media/tikz/Snowflake.png]]
# @@latex: \includestandalone[]{./media/tikz/Snowflake}


#+NAME: testtikzstd
#+CAPTION: This is an example of embedded tikz
#+attr_html: :width 400px
#+attr_latex: :width 12cm
@@latex: \includestandalone[]{./media/tikz/Snowflake}
# [[file:media/tikz/Snowflake.png]]



** Heading 2
   :PROPERTIES:
   :CUSTOM_ID: heading-2
   :END:

*** Heading 3
    :PROPERTIES:
    :CUSTOM_ID: heading-3
    :END:

#+BEGIN_SRC sh
  echo "Hello World"
#+END_SRC


**** Heading 4
     :PROPERTIES:
     :CUSTOM_ID: heading-4
     :END:


***** Heading 5
      :PROPERTIES:
      :CUSTOM_ID: heading-5
      :END:


****** Heading 6
       :PROPERTIES:
       :CUSTOM_ID: heading-6
       :END:

Arbitrary Code:

#+BEGIN_SRC sh
  n/bash

  # Print Help
  if [ "$1" == "-h" ]; then
      echo "Usage: `basename $0` <Format> <CSS>"
      style=~/Dropbox/profiles/Emacs/org-css/github-org.css
      exit 0
  fi

  # Make a working File from clipboard
  filename=lkjdskjjalkjkj392jlkj
  xclip -o -selection clipboard >> $filename
  LocalFile=$filename.org

  pandoc -s  -f org -t gfm $filename -o $filename

  echo "
  This was converted from `org` to `md` using `pandoc -t gfm` at time:
  $(date --utc +%FT%H-%M-%S)
  " >> $filename

  cat $filename | xclip -selection clipboard
  rm $filename

  nv & disown
  echo "Conversion from Org Successful, MD is in Clipboard"

  exit 0
#+END_SRC
* Appendix

#+NAME: r-den_undir_ba
#+CAPTION: */R/* code to produce an image illustrating the density of a simulated Barabasi-Albert graph, the /Barabasi-Albert/ graph is a good analouge for the link structure of the internet cite:langvilleGooglePageRankScience2012,barabasiPhysicsWeb2001,barabasiScalefreeCharacteristicsRandom2000 see the output in figure [[fig:den_undir_ba]]
#+BEGIN_SRC R :exports both :results output graphics file :file media/DensityUndirectedBA.png
library(Matrix)
library(igraph)
n <- 200
m <- 5
power <- 1
g <- igraph::sample_pa(n = n, power = power, m = m, directed = FALSE)
plot(g)
A <- t(get.adjacency(g))
plot(A)
image(A)


# Create a Plotting Region
par(pty = "s", mai = c(0.1, 0.1, 0.4, 0.1))


# create the image

title=paste0("Undirected Barabassi Albert Graph with parameters:\n Power = ", power, "; size = ", n, "; Edges/step = ", round(m))
image(A, axes = FALSE, frame.plot = TRUE, main = title, xlab = "", ylab = "",  )
#+end_src

#+NAME: fig:den_undir_ba
#+CAPTION: Plot of the adjacency matrix corresponding to a Barabassi-Albert (i.e. /Scale Free/) Graph produced by listing [[r-den_undir_ba]], observe the matrix is quite sparse.
#+attr_html: :width 400px
#+attr_latex: :width 12cm
#+RESULTS[3daa56daec945adae3bb37c7ee10ad97523db634]: r-den_undir_ba
[[file:media/DensityUndirectedBA.png]]
* Graph Diagrams
Graph Diagrams shown in [[#markov]] where produced using ~DOT~ (see cite:DOTLanguage,DOTGraphDescription2020).
* Extensions to this Report
Accellerating the Computatoin of Page Rank cite:langvilleGooglePageRankScience2012

* Footnotes

[fn:bvl] NOTE: Similar to a signmoid function, which is a solution to $p \propto p(1-p)$, I wonder if this provides a connection to the exponential nature of the power walk
`﻿erdos.renyi﻿`﻿erdos.renyi﻿``

* TODO  Is the power Walk transition prob matrix a stochastic because it may contain negatives?
* TODO Look at the Trace of the Matrix as a comparison point
* TODO TODO Diamater
Diamater of the web sounds like a fun read cite:albertDiameterWorldWideWeb1999
* TODO Improving the Performance of Page Rank

This:

#+begin_quote
Another approach involves involves reordering the problem and taking advantage
of the fact that the transition probability matrix is sparse  in order
to produce a new algorithm which cannot perform worse than the /power method/
but has been shown to improve the rate of convergence in certain cases.
cite:langvilleReorderingPageRankProblem2006.
#+end_quote


There was also a book that I downloaded that mentioned it
