  #+TITLE: Data Sci Discover Project
:PREAMBLE:
#+OPTIONS: broken-links:auto
#+STARTUP: overview noinlineimages
#+INFOJS_OPT: view:showall toc:3
#+PLOT: title:"Citas" ind:1 deps:(3) type:2d with:histograms set:"yrange [0:]"
#+OPTIONS: tex:t
#+TODO: TODO IN-PROGRESS WAITING DONE
#+CATEGORY: DProj
:END:
:HTML:
#+INFOJS_OPT: view:info toc:3
#+HTML_HEAD_EXTRA: <link rel="stylesheet" type="text/css" href="resources/bigblow.css">
#+CSL_STYLE: /home/ryan/Templates/CSL/nature.csl
:END:
:R:
#+PROPERTY: header-args:R :session DiscProj :dir ./ :cache yes :eval never-export :exports both :results output
:END:
:LATEX:
#+LATEX_HEADER: \IfFileExists{./resources/style.sty}{\usepackage{resources/style}}{}
#+LATEX_HEADER: \IfFileExists{./resources/referencing.sty}{\usepackage{resources/referencing}}{}
#+LATEX_HEADER: \addbibresource{resources/references.bib}
# #+LATEX_HEADER: \twocolumn
:END:

* Links
- [[file:Proposal/Propsal.org][Research Proposal]]
- [[file:laparkPowerWalk2013.pdf][Paper]]
- [[file:ImplementingPageRank/01PageRank.Rmd::---][Preliminary Implementation]]
- [[file:ImplementingPageRank/01PageRank.R][Implement Random Surfer on Sparse Matrix]]
- [[file:ImplementingPowerWalk/01PowerWalk.R][Implement Power Walk on Sparse Matrix]]
* Proposal
** Question

/Can we determine the second eigenvalue from the method parameters? For PageRank, the second eigenvalue is equal to the smoothing parameter \alpha/

#+begin_quote
Yes. An open question for the Power Walk method is, can we determine the second eigenvalue from the method parameters? For PageRank, the second eigenvalue is equal to the smoothing parameter \alpha. The second eigenvalue determines how long the algorithm takes to converge and how stable the solution is.
To begin, implement the method for computing PageRank and then the Power Walk. It can all be done using sparse matrices, so it only requires a fraction of the memory and is each iteration is quick.
#+end_quote

** Working

Take the exemplar Graph from Figure 1:


#+NAME: DotLib
#+CAPTION: Code to Generate DOT Graph
#+begin_src plantuml :output results :file ./Media/Example.png :eval never-export
# #+begin_src javascript :exports code
@startdot
strict digraph graphName {
concentrate=true
fillcolor=green
color=blue
style="filled, rounded"
 A [shape=box, fillcolor="#a31621", style="rounded, filled"]

 edge [
    arrowhead="none"
  ];

 node[
    fontname="Fira Code",
    shape="square",
    fixedsize=false,
    style=rounded
  ];


# A -> B [dir="both"]
A -> B
B [shape=box, fillcolor="#bfdbf7", style="rounded, filled"]
B -> A
C [shape=box, fillcolor="#eaf4d3", style="rounded, filled"]
C -> D
D [shape=box, fillcolor="#0f5257", style="rounded, filled"]
D -> C
}
@enddot
#+end_src

#+attr_html: :width 400px
#+attr_latex: :width 7cm
#+RESULTS: DotLib
[[file:./Media/Example.png]]



\begin{align}
    \Gamma =  I - n D^{- 1}_B \\
\end{align}

Where we have the following:

\begin{align}
    \beta &= 10 \\
    B &= \beta^A \\
    A &=
    \begin{bmatrix}
0& 1& 0& 0 \\
1& 0& 0& 0 \\
0& 0& 0& 1 \\
0& 0& 1& 0
    \end{bmatrix} \\
     \implies
    B &= \begin{bmatrix}
     10 & 1 & 1 & 1 \\
     1 & 10 & 1 & 1 \\
     1 & 1 & 10 & 1 \\
     1 & 1 & 1 & 10 \\
     \end{bmatrix}  \\
     \text{$D_B$ is a diagonal matrix of the column sums:}\\
     D &= \begin{bmatrix}
     13 & 0 & 0 & 0 \\
     0 & 13 & 0 & 0 \\
     0 & 0 & 13 & 0 \\
     0 & 0 & 0 & 13
     \end{bmatrix}  \\
     \text{Hence the Inverse is:}\\
     D_B^{-1}&= \frac{I}{13}\\
     \text{Putting it all together:}\\
     \Gamma &=  I - n D^{- 1}_B \\
     &= I - \frac{4 \cdot I}{13} \\
     &= \frac{9}{13} \cdot  I \\
     &= \begin{bmatrix}
         \frac{9}{13} & 0 & 0 & 0 \\
         0 & \frac{9}{13} & 0 & 0 \\
         0 & 0 & \frac{9}{13} & 0 \\
         0 & 0 & 0 &  \frac{9}{13}
     \end{bmatrix}  \\
     & \approx \begin{bmatrix}
         0.6923 & 0 & 0 & 0 \\
         0 & 0.6923 & 0 & 0 \\
         0 & 0 & 0.6923 & 0 \\
         0 & 0 & 0 & 0.6923
     \end{bmatrix}
\end{align}

* Page Rank Methods
** Introduction
These asses node centrality by performing a random walk across the graph and recording the frequencies of landing on a given vertex.

Usually Page Rank refers to the random surfer but I'm using it in this document to refer to any process that attributes a probability of landing on a vertex during a random walk to a graph that is not ergodic.

If each vertex is connected the graph is said to be ergodic and there is a closed solution for the limit values of the frequencies given this random walk:

- The eigenvalue equal to 1
- If the graph is not directed $\vec{p}$ is a vector of length $n$:
  - $n$ is the number of nodes in the graph $G$
  - $\vec{p}_{i} = \frac{\mathrm{deg}(v_{1})}{\mathrm{vol}(G)}$
    - $\mathrm{vol}(G) = \sum^{n}_{i = 1} \left[ \mathrm{indeg}(v) \right] = \sum^{n}_{i = 1} \left[ \mathrm{outdeg}(v) \right ] = \sum^{n}_{i = 1} \left[ \mathrm{deg}(v) \right]$


For large matrices calculating the eigenvalues will be expensive and so instead the power method is used, which is essentially looping over until the vector converges to a solution.

\begin{align}
\vec{p} = \mathrm{T}\vec{p} \label{eq:pageRank-Method}
\end{align}
where:

+ $\mathrm{A}$ :: Is the adjacency Matrix, an element is 1 if movement from the row vertex to the column vertex is permitted.
  + The matrix may be weighted in some way, for example 5 edges between vertices may be such that a 5 is used in the matrix not a 1
  + An undirected graph will be such that $\mathbf{A} = \mathbf{A}^{\mathrm{\mathbf{T}}}$
+ $\mathrm{T}$ :: Is the transition probability matrix, an element in the matrix describes the probability of moving from the column-vertex to the row-vertex
  + The transition matrix is intended to be such that for a given state distribution $\vec{p}$, the next iteration of a random walk will be $\mathrm{T}\vec{p}$
  + Observe also that $\mathrm{T} = \mathrm{T} \cdot \mathrm{diag}(\mathtt{colsums}(\mathrm{A^{\mathrm{T}}}))$
    - i.e. the transpose of the adjacency matrix with each column scaled to 1.

** Random Surfer
*** Introduction
For con If a graph is non-ergodic, then a random walk isn't as easy to implement
because in escence there are multiple disconnected graphs, to address this, some
value $\lambda$ is introduces which represents the probability of moving from
one vertex to any other vertex. Essentially the difference here is

#+BEGIN_SRC R
  if (require("pacman")) {
      library(pacman)
    }else{
      install.packages("pacman")
      library(pacman)
    }
#+END_SRC

#+BEGIN_SRC R
    pacman::p_load(tidyverse, Matrix, igraph, plotly, mise, docstring)
#+END_SRC

*** Small Graph, Ordinary Matrices
  :PROPERTIES:
  :CUSTOM_ID: implementing-page-rank-methods
  :END:
**** Example Graph
   :PROPERTIES:
   :CUSTOM_ID: example-graph
   :END:

Consider the following Graph taken from the paper:

#+BEGIN_SRC R
  g1 <- igraph::graph.formula(1++2, 1+-8, 1+-5, 2+-5, 2+-7, 2+-8, 2+-6, 2+-9, 3++4, 3+-5, 3+-6, 3+-9, 3+-10, 4+-9, 4+-10, 4+-5, 5+-8, 6+-8, 7+-8)
  plot(g1)
#+END_SRC

[[file:ImplementingPageRank/01PageRank_files/figure-html/unnamed-chunk-2-1.png]]

***** Adjacency Matrix
    :PROPERTIES:
    :CUSTOM_ID: adjacency-matrix
    :END:

The adjacency Matrix is given by:

#+BEGIN_SRC R
  A <- igraph::get.adjacency(g1, names = TRUE, sparse = FALSE) %>%
    as.matrix()

  ## Adjust the Order
  (A <- A[order(as.integer(row.names(A))), order(as.integer(colnames(A)))])
#+END_SRC

#+BEGIN_EXAMPLE
  ##    1 2 3 4 5 6 7 8 9 10
  ## 1  0 1 0 0 0 0 0 0 0  0
  ## 2  1 0 0 0 0 0 0 0 0  0
  ## 3  0 0 0 1 0 0 0 0 0  0
  ## 4  0 0 1 0 0 0 0 0 0  0
  ## 5  1 1 1 1 0 0 0 0 0  0
  ## 6  0 1 1 0 0 0 0 0 0  0
  ## 7  0 1 0 0 0 0 0 0 0  0
  ## 8  1 1 0 0 1 1 1 0 0  0
  ## 9  0 1 1 1 0 0 0 0 0  0
  ## 10 0 0 1 1 0 0 0 0 0  0
#+END_EXAMPLE

***** State Distribution
    :PROPERTIES:
    :CUSTOM_ID: state-distribution
    :END:

The state distribution is the transpose of the adjacency matrix:

#+BEGIN_SRC R
  (p0 <- t(A))
#+END_SRC

#+BEGIN_EXAMPLE
  ##    1 2 3 4 5 6 7 8 9 10
  ## 1  0 1 0 0 1 0 0 1 0  0
  ## 2  1 0 0 0 1 1 1 1 1  0
  ## 3  0 0 0 1 1 1 0 0 1  1
  ## 4  0 0 1 0 1 0 0 0 1  1
  ## 5  0 0 0 0 0 0 0 1 0  0
  ## 6  0 0 0 0 0 0 0 1 0  0
  ## 7  0 0 0 0 0 0 0 1 0  0
  ## 8  0 0 0 0 0 0 0 0 0  0
  ## 9  0 0 0 0 0 0 0 0 0  0
  ## 10 0 0 0 0 0 0 0 0 0  0
#+END_EXAMPLE

***** Probability Transition Matrix
    :PROPERTIES:
    :CUSTOM_ID: probability-transition-matrix
    :END:

The probability transition matrix is such that each column of the
initial state distribution (i.e. the transposed adjacency matrix) is
scaled to 1.

#+BEGIN_SRC R
  p0 %*% diag(1/colSums(p0))
#+END_SRC

#+BEGIN_EXAMPLE
  ##    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]      [,9] [,10]
  ## 1     0    1    0    0 0.25  0.0    0  0.2 0.0000000   0.0
  ## 2     1    0    0    0 0.25  0.5    1  0.2 0.3333333   0.0
  ## 3     0    0    0    1 0.25  0.5    0  0.0 0.3333333   0.5
  ## 4     0    0    1    0 0.25  0.0    0  0.0 0.3333333   0.5
  ## 5     0    0    0    0 0.00  0.0    0  0.2 0.0000000   0.0
  ## 6     0    0    0    0 0.00  0.0    0  0.2 0.0000000   0.0
  ## 7     0    0    0    0 0.00  0.0    0  0.2 0.0000000   0.0
  ## 8     0    0    0    0 0.00  0.0    0  0.0 0.0000000   0.0
  ## 9     0    0    0    0 0.00  0.0    0  0.0 0.0000000   0.0
  ## 10    0    0    0    0 0.00  0.0    0  0.0 0.0000000   0.0
#+END_EXAMPLE

****** Create a Function
     :PROPERTIES:
     :CUSTOM_ID: create-a-function
     :END:

#+BEGIN_SRC R
  adj_to_probTrans <- function(adjMat) {
    t(adjMat) %*% diag(1/colSums(t(adjMat)))
  }

  (T <- adj_to_probTrans(A)) %>% round(2)
#+END_SRC

#+BEGIN_EXAMPLE
  ##    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
  ## 1     0    1    0    0 0.25  0.0    0  0.2 0.00   0.0
  ## 2     1    0    0    0 0.25  0.5    1  0.2 0.33   0.0
  ## 3     0    0    0    1 0.25  0.5    0  0.0 0.33   0.5
  ## 4     0    0    1    0 0.25  0.0    0  0.0 0.33   0.5
  ## 5     0    0    0    0 0.00  0.0    0  0.2 0.00   0.0
  ## 6     0    0    0    0 0.00  0.0    0  0.2 0.00   0.0
  ## 7     0    0    0    0 0.00  0.0    0  0.2 0.00   0.0
  ## 8     0    0    0    0 0.00  0.0    0  0.0 0.00   0.0
  ## 9     0    0    0    0 0.00  0.0    0  0.0 0.00   0.0
  ## 10    0    0    0    0 0.00  0.0    0  0.0 0.00   0.0
#+END_EXAMPLE

**** Page Rank Random Surfer
   :PROPERTIES:
   :CUSTOM_ID: page-rank-random-surfer
   :END:

The random surfer page rank method modifies the probability transition
matrix $T$ so that the method works also for non-ergodic graphs by
introducing the possibility of a random jump, we'll call the surfer
transition matrix $S$:

\begin{align}
    S &= \lambda T +  \left( 1- \lambda \right)B :\\
\ \\
    B&= \begin{bmatrix}
    \frac{1}{N} & \frac{1}{N} & \ldots & \frac{1}{N} \\
    \frac{1}{N} & \frac{1}{N} & \ldots & \frac{1}{N} \\
        \vdots      & \vdots      & \ddots & \vdots  \\
    \frac{1}{N} & \frac{1}{N} & \ldots & \frac{1}{N} \\
    \end{bmatrix} \label{eq:bgval1} \\
    N&= \left| \left| V \right| \right| \\
    \lambda &\in [0,1]
\end{align}

#+BEGIN_SRC R
  B <- matrix(rep(1/nrow(T), length.out = nrow(T)**2), nrow = nrow(T))
  l <- 0.8123456789

  S <- l*T+(1-l)*B
#+END_SRC

***** Eigen Value Method
    :PROPERTIES:
    :CUSTOM_ID: eigen-value-method
    :END:

The eigenvector corresponding to the the eigenvalue of 1 will be the
stationary point:

#+BEGIN_SRC R
  eigen(S, symmetric = FALSE)
#+END_SRC

#+begin_example
eigen() decomposition
$values
 [1]  1.000000e+00 -8.123457e-01 -8.123457e-01  8.123457e-01 -3.407464e-09  3.407464e-09
 [7]  6.878591e-17 -4.393838e-17 -1.126771e-18 -1.292735e-32

$vectors
            [,1]          [,2]          [,3]          [,4]          [,5]          [,6]
 [1,] 0.48726141 -7.071005e-01  1.590774e-03  5.000000e-01  6.735753e-01 -6.735753e-01
 [2,] 0.52676629  7.071005e-01 -1.590774e-03  5.000000e-01  9.622504e-02 -9.622505e-02
 [3,] 0.49149620 -2.975837e-03  7.071050e-01 -5.000000e-01  9.622504e-02 -9.622505e-02
 [4,] 0.48044122  2.975837e-03 -7.071050e-01 -5.000000e-01  2.886751e-01 -2.886751e-01
 [5,] 0.04932738  1.463673e-18 -5.541166e-17  2.124631e-17 -3.849002e-01  3.849002e-01
 [6,] 0.04932738  1.463673e-18  5.541166e-17  2.124631e-17 -3.849002e-01  3.849002e-01
 [7,] 0.04932738  1.463673e-18 -2.077937e-17  2.124631e-17 -3.849002e-01  3.849002e-01
 [8,] 0.04243328 -6.484884e-18 -1.103904e-17  6.319692e-17  8.072508e-09  8.072508e-09
 [9,] 0.04243328  6.952446e-18 -9.740331e-18  6.005334e-17  8.072508e-09  8.072509e-09
[10,] 0.04243328  6.952446e-18 -9.740331e-18  6.005334e-17  8.072508e-09  8.072509e-09
               [,7]          [,8]          [,9]         [,10]
 [1,] -3.963430e-01  3.962600e-01  1.828019e-01 -1.752367e-01
 [2,] -1.291621e-01  2.027302e-01  2.199538e-01 -2.197680e-01
 [3,] -3.955284e-01  3.894308e-02  2.223048e-01 -2.248876e-01
 [4,] -4.215353e-01  1.043870e-01  2.747562e-01 -2.777266e-01
 [5,]  5.166485e-01 -8.109210e-01 -8.798152e-01  8.790721e-01
 [6,]  5.201366e-02 -1.308878e-01 -1.049028e-01  1.056778e-01
 [7,]  1.346275e-01 -1.936007e-01  9.054366e-02 -9.554811e-02
 [8,]  2.547528e-16 -1.352936e-16 -1.025353e-16  1.072771e-16
 [9,]  3.196396e-01  1.965446e-01 -2.821213e-03 -5.466313e-03
[10,]  3.196396e-01  1.965446e-01 -2.821213e-03  1.388344e-02

#+end_example

So in this case the stationary point is

$\langle -0.49, -0.53, -0.49, -0.48, -0.05, -0.05, -0.05, -0.04, -0.04, -0.04 \rangle$

which can be verified:

$$
1 \vec{p} = S\vec{p}
$$

#+BEGIN_SRC R
  (p     <- eigen(S)$values[1] * eigen(S)$vectors[,1])
#+END_SRC

#+BEGIN_EXAMPLE
  ##  [1] -0.48531271 -0.52732002 -0.49152601 -0.47977477 -0.05288058 -0.05288058
  ##  [7] -0.05288058 -0.04558671 -0.04558671 -0.04558671
#+END_EXAMPLE

#+BEGIN_SRC R
  (p_new <- S %*% p)
#+END_SRC

#+BEGIN_EXAMPLE
  ##           [,1]
  ## 1  -0.48531271
  ## 2  -0.52732002
  ## 3  -0.49152601
  ## 4  -0.47977477
  ## 5  -0.05288058
  ## 6  -0.05288058
  ## 7  -0.05288058
  ## 8  -0.04558671
  ## 9  -0.04558671
  ## 10 -0.04558671
#+END_EXAMPLE

However this vector does not sum to 1 so the scale should be adjusted
(for probabilities the vector should sum to 1):

#+BEGIN_SRC R
  (p_new <- p_new/sum(p_new))
#+END_SRC

#+BEGIN_EXAMPLE
  ##         [,1]
  ## 1  0.2129185
  ## 2  0.2313481
  ## 3  0.2156444
  ## 4  0.2104889
  ## 5  0.0232000
  ## 6  0.0232000
  ## 7  0.0232000
  ## 8  0.0200000
  ## 9  0.0200000
  ## 10 0.0200000
#+END_EXAMPLE

***** Power Value Method
    :PROPERTIES:
    :CUSTOM_ID: power-value-method
    :END:

Using the power method should give the same result, which it indeed
does, but for the scale:

#+BEGIN_SRC R
  p_new <- p_new *123456789

  while (sum(round(p, 9) != round(p_new, 9))) {
      (p     <- p_new)
      (p_new <- S %*% p)
  }

  p_new
#+END_SRC

#+BEGIN_EXAMPLE
  ##        [,1]
  ## 1  26286237
  ## 2  28561500
  ## 3  26622771
  ## 4  25986282
  ## 5   2864198
  ## 6   2864198
  ## 7   2864198
  ## 8   2469136
  ## 9   2469136
  ## 10  2469136
#+END_EXAMPLE

#+BEGIN_SRC R
  p
#+END_SRC

#+BEGIN_EXAMPLE
  ##        [,1]
  ## 1  26286237
  ## 2  28561500
  ## 3  26622771
  ## 4  25986282
  ## 5   2864198
  ## 6   2864198
  ## 7   2864198
  ## 8   2469136
  ## 9   2469136
  ## 10  2469136
#+END_EXAMPLE

This answer is however identical in direction, if it scaled to 1 the
same value will be returned:

#+BEGIN_SRC R
  (p_new <- p_new/sum(p_new))
#+END_SRC

#+BEGIN_EXAMPLE
  ##         [,1]
  ## 1  0.2129185
  ## 2  0.2313481
  ## 3  0.2156444
  ## 4  0.2104889
  ## 5  0.0232000
  ## 6  0.0232000
  ## 7  0.0232000
  ## 8  0.0200000
  ## 9  0.0200000
  ## 10 0.0200000
#+END_EXAMPLE

***** Scaling
    :PROPERTIES:
    :CUSTOM_ID: scaling
    :END:

However if the initial state sums to 1, then the scale of the stationary
vector will also sum to 1.

#+BEGIN_SRC R
  p     <- c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0)
  p_new <- S %*% p

  while (sum(round(p, 9) != round(p_new, 9))) {
      (p     <- p_new)
      (p_new <- S %*% p)
  }

  cbind(p_new, p)
#+END_SRC

#+BEGIN_EXAMPLE
  ##         [,1]      [,2]
  ## 1  0.2129185 0.2129185
  ## 2  0.2313481 0.2313481
  ## 3  0.2156444 0.2156444
  ## 4  0.2104889 0.2104889
  ## 5  0.0232000 0.0232000
  ## 6  0.0232000 0.0232000
  ## 7  0.0232000 0.0232000
  ## 8  0.0200000 0.0200000
  ## 9  0.0200000 0.0200000
  ## 10 0.0200000 0.0200000
#+END_EXAMPLE
*** Large Graph, Sparse Matrices using CRS
**** Creating the Probability Transition Matrix
Implementing the page rank method on a larger graph requires the use of more efficient form of matrix storage.

An adjacency matrix (atleast in the context of graphs relating to webpages and social networks) will contain elements that are mostly zero because the number of edges leaving any vertex will tend to be significantly less than the total number of vertices.

A matrix exhibiting this property is known as a sparse matrix CITE

The properties of a sparse matrix can be implemented in order to improve performance, one such method to acheive this is /Compressed Sparse Row/ (CSR) storage, which involves creating a seperate array of values and corresponding indices. CITE

This is implemented by the Matrix package in */R/*. CITE

An sparse matrix can be created using the following syntax, which will return a matrix of the class ~dgCMatrix~:

#+begin_src R :results output
library(Matrix)
## Create Example Matrix
n <- 20
m <- 10^6
i <- sample(1:m, size = n); j <- sample(1:m, size = n); x <- rpois(n, lambda = 90)
A <- sparseMatrix(i, j, x = x, dims = c(m, m))

summary(A)
#+end_src

#+RESULTS[26753ee076e693bdce4667779622cffec7f8d950]:
#+begin_example

1000000 x 1000000 sparse Matrix of class "dgCMatrix", with 20 entries
        i      j   x
1  803589  66922 118
2   61426  83355  97
3  401058 103999  71
4  610432 206922  84
5  542888 217196  69
6  821769 291405  79
7  187782 364814  74
8  152229 451810 104
9  614645 462031  82
10 776459 566334  91
11 288279 630438  97
12 233553 631441  84
13 139900 649740  83
14 381442 681415  87
15 578270 755635  99
16 175521 775788  98
17  57981 809115  89
18 821120 809688 103
19 541818 976802  78
20 595348 993420  85
#+end_example

As before in section [[#probability-transition-matrix]], the probability transition matrix can be found by:

1. Transposing the adjacency matrix, then
2. Scaling the columns to one

To implement this for a sparseMatrix of the class ~dgCMatrix~, the same technique of multiplying by a diagonalised matrix may be implemented, however to create this new matrix, a new ~sparseMatrix~ will need to be created using the properties of the original matrix, this can be done like so:


#+begin_src R :results output
 sparse_diag <- function(mat) {
  #' Diagonal Factors of Sparse Matrix
  #'
  #' Return a Diagonal Matrix of the 1 / colsum() such that
  #' matrix multiplication with this matrix would have all column sums
  #' sum to 1
  #'
  #' This should take the transpose of an adjacency matrix in and the output
  #' can be multiplied by the original matrix to scale it to 1.
  #' i

  ## Get the Dimensions
  n <- nrow(mat)

  ## Make a Diagonal Matrix of Column Sums
  D <- sparseMatrix(i = 1:n, j = 1:n, x = colSums(mat), dims = c(n,n))

  ## Throw away explicit Zeroes
  D <- drop0(D)

  ## Inverse the Values
  D@x <- 1/D@x

  ## Return the Diagonal Matrix
  return(D)
}
D <- sparse_diag(t(A))
summary(D)
#+end_src

#+RESULTS[175b4834319aa086de0fe8c242730344f4f80681]:
#+begin_example

1000000 x 1000000 sparse Matrix of class "dgCMatrix", with 20 entries
        i      j           x
1   57981  57981 0.011235955
2   61426  61426 0.010309278
3  139900 139900 0.012048193
4  152229 152229 0.009615385
5  175521 175521 0.010204082
6  187782 187782 0.013513514
7  233553 233553 0.011904762
8  288279 288279 0.010309278
9  381442 381442 0.011494253
10 401058 401058 0.014084507
11 541818 541818 0.012820513
12 542888 542888 0.014492754
13 578270 578270 0.010101010
14 595348 595348 0.011764706
15 610432 610432 0.011904762
16 614645 614645 0.012195122
17 776459 776459 0.010989011
18 803589 803589 0.008474576
19 821120 821120 0.009708738
20 821769 821769 0.012658228
#+end_example

and hence the probability transition matrix may be implemented by performing matrix multiplication accordingly:

#+begin_src R :results output
summary(t(A) %*% D)
#+end_src

#+RESULTS[ad7d15da22594d5a66fdf838525c092c6f5f93e7]:
#+begin_example
1000000 x 1000000 sparse Matrix of class "dgCMatrix", with 20 entries
        i      j x
1  809115  57981 1
2   83355  61426 1
3  649740 139900 1
4  451810 152229 1
5  775788 175521 1
6  364814 187782 1
7  631441 233553 1
8  630438 288279 1
9  681415 381442 1
10 103999 401058 1
11 976802 541818 1
12 217196 542888 1
13 755635 578270 1
14 993420 595348 1
15 206922 610432 1
16 462031 614645 1
17 566334 776459 1
18  66922 803589 1
19 809688 821120 1
20 291405 821769 1
#+end_example

**** Solving the Random Surfer via the Power Method
Solving the eigenvalues for such a large matrix will not feasible, instead the power method will need to be used to find the stationary point.

However, creating a matrix of background probabilites (denoted by ~B~ is section [[#page-rank-random-surfer]]) will not be feasible, it would simply be too large, instead some algebra can be used to reduce $B$ from a matrix into a vector containing only $\frac{1-\alpha}{N}$.

The power method is given by:

\begin{align}
\vec{p}= \mathbf{S} \vec{p}
\end{align}

where:

\begin{align}
S &= \alpha \mathbf{T} +  \left( 1 - \alpha \right) \mathbf{B} \\
\vec{p} &= \left( \alpha \mathbf{T} +  \left( 1 - \alpha \right) \mathbf{B} \right) \vec{p}\\
&= \alpha \mathbf{T}\vec{p} +  \left( 1-\alpha \right) \mathbf{B} \vec{p}
\end{align}

Let $\mathbf{F}= \mathbf{B}\vec{p}$, consider the value of $\mathbf{F}$ :

\begin{align}
\mathbf{F} &=
\begin{bmatrix}
\frac{1}{N} & \frac{1}{N} & \ldots & \frac{1}{N} \\
\frac{1}{N} & \frac{1}{N} & \ldots & \frac{1}{N} \\
\vdots      & \vdots      & \ddots & \vdots \\
\frac{1}{N} & \frac{1}{N} & \ldots & \frac{1}{N} \\
\end{bmatrix} \label{eq:bgVal2}
\begin{bmatrix}
\vec{p_1} \\ \vec{p_2} \\ \vdots \\ \vec{p_m}
\end{bmatrix}  \\
&= \begin{bmatrix}
\left( \sum^{m}_{i= 0}   \left[ p_i \right]  \right) \times \frac{1}{N} \\
\left( \sum^{m}_{i= 0}   \left[ p_i \right]  \right) \times \frac{1}{N} \\
\vdots  \\
\left( \sum^{m}_{i= 0}   \left[ p_i \right]  \right) \times \frac{1}{N} \\
\end{bmatrix}  \\
& \text{Probabilities sum to 1 and hence:} \\
&= \begin{bmatrix}
\frac{1}{N} \\
\frac{1}{N} \\
\frac{1}{N} \\
\vdots  \\
\frac{1}{N} \\
\end{bmatrix}
\end{align}
So instead the power method can be implemented by performing an algorithm to the effect of:

#+begin_src R
## Find Stationary point of random surfer
N     <- nrow(A)
alpha <- 0.8
F     <- rep((1-alpha)/N, nrow(A))  ## A nx1 vector of (1-alpha)/N

## Solve using the power method
p     <- rep(0, length.out = ncol(T)); p[1] <- 1
p_new <- alpha*T %*% p + F

## use a Counter to debug
i <- 0
while (sum(round(p, 9) != round(p_new, 9))) {
    p     <- p_new
    p_new <- alpha*T %*% p + F
    (i <- i+1) %>% print()
}

p %>% head() %>% print()
#+end_src
** Power Walk Method
*** Introduction

\begin{align}
\mathbf{T} &= \mathbf{B} \mathbf{D}^{-1}_{B} \label{eq:pwalk-def}
\end{align}



where:

- $\mathbf{B}= \beta^{\mathbf{A}}$
  - $x\beta^{1}$  :: probability of following an edge of weight 1
  - $x\beta^{0}$  :: probability of following an edge of weight 0
  - $x\beta^{-1}$ :: probability of following an edge of weight -
- $D_{B} = \mathtt{colsums}(\mathbf{B})$
- $\mathbf{A}$ :: The Adjacency Matrix

*** Ordinary Matrices
Solving the Power walk can be done pretty much the same as it is with the random surfer, but doing it with Sparse Matrices is a bit trickier.
*** Sparse Matrices
**** Theory; Simplifying Power Walk to be solved with Sparse Matrices
The Random Surfer model is:

$$\begin{aligned}
    \mathbf{S} &= \alpha \mathbf{T} +  \mathbf{F}  \label{eq:sparse-RS}\end{aligned}$$

where:

- $\mathbf{T}$

  - is an $i \times j$ matrix that describes the probability of
    travelling from vertex $j$ to $i$

    - This is transpose from the way that =igraph= produces an adjacency
      matrix.

- $\mathbf{F} = \begin{bmatrix} \frac{1}{n} \\ \frac{1}{n} \\ \frac{1}{n} \vdots \end{bmatrix}$

Interpreting the transition probability matrix in this way is such that
$\mathbf{T}= \mathbf{A}\mathbf{D}^{- 1}_A$ under the following
conditions:


- No column of $\mathbf{A}$ sums to zero

  - If this does happen the question arises how to deal with
    $\mathbf{D_\mathbf{A}^{- 1}}$

    - I've been doing $\mathbf{D}^{\mathrm{T}}_{\mathbf{A}, i, j} := \mathtt{diag} \left( {\frac{1}{\mathtt{colsums}\left( \mathbf{A} \right)}} \right)$
      and then replacing any $0$ on the diagonal with 1.

  - What is done in the paper is to make another matrix $\mathbf{Z}$
    that is filled with 0, if a column sum of $\mathbf{A}$ adds to zero
    then that column in $\mathbf{Z}$ becomes $\frac{1}{n}$

    - This has the effect of making each row identical

    - The probability of going from an orphaned vertex to any other
      vertex would hence be $\frac{1}{n}$

    - The idea with this method is then to use
      $D_\mathbf{\left( A+Z \right)}^{- 1}$ this will be consistent with
      the /Random Surfer/ the method using $\mathbf{F}$ in
      [[#eq:sparse-RS][]] eqref:eq:sparse-RS

    where each row is identical that is a 0

The way to deal with the /Power Walk/ is more or less the same.

observe that:

$$\begin{aligned}
   \left( \mathbf{B} = \beta^{\mathbf{A}} \right)\wedge \left( \mathbf{A}_{i, j}\right)\in \mathbb{R}  \implies  \left\lvert \mathbf{B}_{i, j} \right\rvert > 0 \quad \forall i,j>n\in \mathbb{Z}^+ \label{eq:b-is-pos}\end{aligned}$$



Be mindful that the use of exponentiation in
[[#eq:b-is-pos][[eq:b-is-pos]]] is not an element wise exponentiation
and not an actual matrix exponential (which would be defined by using
power series and logs but is defined)

So if I have:

- $\mathbf{O}_{i, j} := 0, \quad \forall i,j\leq n \in \mathbb{Z}^+$

- $\vec{p_i}$ as the state distribution, being a vector of length $n$

Then It can be shown (see eqref:eq:sparse-power-walk):

$$\begin{aligned}
    \mathbf{O} \mathbf{D}_{\mathbf{B}}^{-1} \vec{p_i} = \mathtt{repeat} (\vec{p} \bullet \vec{\delta^{\tiny \mathrm{T}}} \mathtt{, n}\end{aligned})$$



where:

- $\vec{\delta_i} = \frac{1}{\mathtt{colsums} \left( \mathbf{B} \right)}$
  + A vector...($n\times 1$ matrix)
- $\vec{1}$  :: is a vector containing all 1's
  + A vector...($n\times 1$ matrix)
- $\vec{\delta^{\mathrm{T}}}$ :: refers to the transpoxe of $\vec{\detla}$ ($1\times n$ matrix)
- $\vec{\delta^{\mathrm{T}}} \vec{p_{i}}$ :: is some number (because it's a dot product)

This means we can do:

\begin{align}
  \overrightarrow{p_{i + 1}} & = \mathbf{T}_{\mathrm{pw}}
  \overrightarrow{p_i}\\
& = \mathbf{BD}_{\mathbf{B}}^{- 1}
  \overrightarrow{p_i}\\
  & = \left( \mathbf{B} - \mathbf{O} + \mathbf{O} \right)
  \mathbf{D}_{\mathbf{B}}^{- 1} \overrightarrow{p_i}\\
  & = \left( \left( \mathbf{B} - \mathbf{O} \right)
  \mathbf{D}_{\mathbf{B}}^{- 1} + \mathbf{OD}_{\mathbf{B}}^{- 1} \right)
  \overrightarrow{p_i}\\
  & = \left( \mathbf{B} - \mathbf{O} \right) \mathbf{D}_{\mathbf{B}}^{- 1}
  \overrightarrow{p_i} + \mathbf{OD}_{\mathbf{B}}^{- 1} \overrightarrow{p_i}\\
  & = \left( \mathbf{B} - \mathbf{O} \right) \mathbf{D}_{\mathbf{B}}^{- 1}
  \overrightarrow{p_i} + \vec{1} (\overrightarrow{\delta^{\mathrm{T}}}
  \overrightarrow{p_i}) \\
  & = \left( \mathbf{B} - \mathbf{O} \right) \mathbf{D}_{\mathbf{B}}^{- 1}
  \overrightarrow{p_i} + \mathtt{rep} (\overrightarrow{\delta^{\mathrm{T}}}
  \overrightarrow{p_i})
\end{align}

where:


Let $(\mathbf{B}-\mathbf{O}) = \mathbf{B_{\mathbf{O}}}$:

\begin{eqnarray*}
  \overrightarrow{p_{i + 1}} & = \mathbf{B_{\mathbf{O}}} \mathbf{D}_{\mathbf{B}}^{- 1}
  \overrightarrow{p_i} + \mathtt{rep} (\overrightarrow{\delta^{\mathrm{T}}}
  \overrightarrow{p_i}) &
\end{eqnarray*}

Now solve $\tmmathbf{D}_B^{- 1}$ in terms of $\mathbf{B_{O}}$ :

\begin{align}
  \mathbf{B}_{\mathbf{\mathbf{O}}} = & (\mathbf{B}-\mathbf{O})\\
  \mathbf{B} = & \mathbf{B}_{\mathbf{\mathbf{O}}}
  +\mathbf{O}
\end{align}

If we have $\delta_{\mathbf{B}}$ as the column sums of$\tmmathbf{\Beta}$ $\mathbf{B}$:

\begin{align}
\delta^{-1}_{\mathbf{B}} &= \vec{1}\mathbf{B} \\
&= \vec{1} \left( \mathbf{B_{O}} + \mathbf{O}\right) \\
&= \vec{1}  \mathbf{B_{O}} + \vec{1}\mathbf{O} \\
&= \vec{1} \mathbf{B_{\mathbf{O}}} + \langle n, n, n, ... n \rangle \\
&= \vec{1} \mathbf{B_{\mathbf{O}}} + \vec{1} n \\
\delta_{\mathbf{B}}&=\mathtt{1/(colSums(\mathbf{B_{O}}) + n )}
\end{align}

Then if we have $\mathit{{\tmstrong{{\tmem{D}}}}}_{\mathit{{\tmem{{\tmstrong{B}}}}}} =
\mathtt{diag} (\delta_{\tmmathbf{B}}) \mathtt{}$:


\[ \begin{array}{lll}
     \mathit{{\tmstrong{{\tmem{D}}}}}_{\mathit{{\tmem{{\tmstrong{B}}}}}}^{- 1}
     & = & \mathrm{diag} \left( \delta^{- 1}_{\mathbf{B}} \right)\\
     & = & \mathtt{diag} \left( \mathtt{ColSums}
     (\mathtt{\tmmathbf{B}_{\tmmathbf{O}}}) + \mathtt{n}
     \right)^{\mathtt{- 1}}
   \end{array} \]

And so the the power method can be implemented using sparse matrices:

\begin{align}
\vec{p_{i+1}} = \mathrm{B_{O}} \enspace \mathrm{diag}\left( \vec{1} \mathbf{B_{O}} + \vec{1}n \right) \vec{p_{i}} + \vec{1} \vec{\delta^{\mathrm{T}}\vec{p_{i}}}
\end{align}

in terms of */R/*:

#+begin_src R
p_new <- Bo %*% diag(colSums(B)+n) %*% p + rep(t(δ) %*% p, n)

# It would also be possible to sum the element-wise product
(t(δ) %*% p) == sum(δ * p)

# Because R treats vectors the same as a nX1 matrix we could also
# perform the dot product of the two vectors, meaning the following
# would be true in R but not generally

(t(δ) %*% p) == (δ %*% p)
#+end_src


***** Solving the Background Probability
In this case a vertical single column matrix will represent a vector and $\otimes$ will represent the outer product (i.e. the /Kronecker Product/):



   Define \(\vec{\delta}\) as the column sums of
\[\begin{aligned}
     \vec{\delta} & = \mathtt{colsum} (\text{{\bfseries{B}}})^{- 1}\\
     & = \frac{1}{\overrightarrow{1^{{\scriptsize \ensuremath{\boldsymbol{T}}}}}
     \ensuremath{\boldsymbol{B}}}
   \end{aligned}\]


Then we have:


\[ \begin{aligned}
     \mathbf{OD}_{\mathbf{B}}^{- 1} \overrightarrow{p_i} & = \left(
     \begin{array}{cccc}
       1 & 1 & 1 & \\
       1 & 1 & 1 & \ldots\\
       1 & 1 & 1 & \\
       & \vdots &  & \ddots
     \end{array} \right) \left( \begin{array}{cccc}
       \frac{1}{\delta_1} & 0 & 0 & \\
       0 & \frac{1}{\delta_2} & 0 & \ldots\\
       0 & 0 & \frac{1}{\delta_{13}} & \\
       & \vdots &  & \ddots
     \end{array} \right) \left( \begin{array}{c}
       p_{i, 1}\\
       p_{i, 2}\\
       p_{i, 3}\\
       \vdots
     \end{array} \right) \nonumber \nonumber\\
     & = \left( \begin{array}{cccccc}
       \frac{p_{i, 1}}{\delta 1} & + & \frac{p_{i, 2}}{\delta_2} & + &
       \frac{p_{i, 3}}{\delta_3} & \\
       \frac{p_{i, 1}}{\delta 1} & + & \frac{p_{i, 2}}{\delta_2} & + &
       \frac{p_{i, 3}}{\delta_3} & \ldots\\
       \frac{p_{i, 1}}{\delta 1} & + & \frac{p_{i, 2}}{\delta_2} & + &
       \frac{p_{i, 3}}{\delta_3} & \\
       &  & \vdots &  &  & \ddots
     \end{array} \right) \nonumber \nonumber\\
     & = \left( \begin{array}{c}
       \sum^n_{k = 1} [p_{i, k} \delta_i]\\
       \sum^n_{k = 1} [p_{i, k} \delta_i]\\
       \sum^n_{k = 1} [p_{i, k} \delta_i]\\
       \vdots
     \end{array} \right) \nonumber\\
     & = \left( \begin{array}{c}
       \overrightarrow{\delta^{{\footnotesize \tmmathbf{T}}}}
       \overrightarrow{p_i}\\
       \overrightarrow{\delta^{{\footnotesize \tmmathbf{T}}}} \vec{p}_i\\
       \overrightarrow{\delta^{{\footnotesize \tmmathbf{T}}}} \vec{p}_i\\
       \vdots
     \end{array} \right) \nonumber\\
     & = \overrightarrow{\delta^{{\footnotesize \tmmathbf{T}}}}
     \overrightarrow{p_i} \left( \begin{array}{c}
       1\\
       1\\
       1\\
       \vdots
     \end{array} \right) \nonumber\\
     & = (\overrightarrow{\delta^{{\footnotesize \tmmathbf{T}}}}
     \overrightarrow{p_i})  \vec{1}\\
     & = \mathtt{repeat} (\overrightarrow{\delta} \overrightarrow{p_i}
     \mathtt{, n}) \label{eq:sparse-power-walk}
   \end{aligned} \]
Observe also that If we let $\vec{\delta}$ and $p_i$ be 1 dimensional
vectors, this can also be expressed as a dot product:

   | Matrices                                | Vectors                    |
   | $\vec{\delta^{\mathrm{T}}} \vec{p_{i}}$ | $\vec{\delta} \vec{p_{i}}$ |

**** Practical; Implementing the Power Walk on Sparse Matrices
***** Inspect the newly created matrix and create constants
***** Setup
****** Load Packages
#+begin_src R :session example
if (require("pacman")) {
    library(pacman)
  }else{
    install.packages("pacman")
    library(pacman)
  }
  pacman::p_load(Matrix, igraph, plotly, mise, docstring, expm)
  mise()
#+end_src

#+RESULTS[b1fd4f7af2cadbc1374b0f8d78f62a6ad9342541]:
: Loading required package: pacman

****** Define function to create DiagonalsSparse Diagonal Function
This doesn't matter for the power walk, real exponents will always give non-zero values anyway
#+begin_src R :session example
sparse_diag <- function(mat) {
  #' Diagonal Factors of Sparse Matrix
  #'
  #' Return a Diagonal Matrix containing either 1 / colsum() or 0 such that
  #' matrix multiplication with this matrix would have all columns
  #' sum to 1
  #'
  #' This should take the transpose of an adjacency matrix in and the output
  #' can be multiplied by the original matrix to scale it to 1.
  #' i
  # mat  <- A
  ## Get the Dimensions
  n <- nrow(mat)

  ## Make a Diagonal Matrix of Column Sums
      ## If a column sums to zero the diag can be zero iff the adjacency_matrix>=0
  D <- sparseMatrix(i = 1:n, j = 1:n, x = colSums(mat), dims = c(n,n))

  ## Throw away explicit Zeroes
  D <- drop0(D)

  ## Inverse the Values
  D@x <- 1/D@x

  ## Return the Diagonal Matrix
  return(D)
}
#+end_src

#+RESULTS[b8327916d90bdc5810e057f7de6f3e0808ea7b88]:

****** Make an Example Graph
#+begin_src R :session example
g1 <- igraph::erdos.renyi.game(n = 20, 0.2)
A <- igraph::get.adjacency(g1) # Row to column


beta = 0.843234
β = beta
#+end_src
****** Plot

#+BEGIN_SRC R :exports both :session example :results output graphics file :file ./Media/Example-graph-plot-debug-power-walk.png
plot(g1)
#+END_SRC

#+RESULTS[5eec355fc3d55ed8cd2dd42e0f68ab07c9ca67fa]:
[[file:./Media/Example-graph-plot-debug-power-walk.png]]

***** Power Walk
****** Define B
#+begin_src R :session example
B      <- A
B@x    <- β^(A@x)
B      <- A
B       <- β^A

Bo     <- A

# These two approaches are equivalent
Bo@x   <- β^(A@x) -1   # This in theory would be faster
# Bo     <- β^(A) -1
# Bo     <- drop0(Bo)


  n <- nrow(A)
#+end_src

#+RESULTS[bc515375922834cfac37ab066bfcd2261fe752a0]:

#+begin_src R :session example :results output
print(B)
#+end_src

#+RESULTS[a32b596a74cff397c7bf190d87be4f0fa650f331]:
#+begin_example
20 x 20 Matrix of class "dgeMatrix"
          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]
 [1,] 1.000000 0.843234 1.000000 1.000000 1.000000 0.843234 1.000000 1.000000
 [2,] 0.843234 1.000000 1.000000 1.000000 0.843234 1.000000 1.000000 1.000000
 [3,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
 [4,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234
 [5,] 1.000000 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
 [6,] 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234
 [7,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
 [8,] 1.000000 1.000000 1.000000 0.843234 1.000000 0.843234 1.000000 1.000000
 [9,] 0.843234 1.000000 1.000000 1.000000 0.843234 1.000000 1.000000 1.000000
[10,] 0.843234 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
[11,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
[12,] 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234 1.000000 0.843234
[13,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234
[14,] 1.000000 0.843234 1.000000 0.843234 1.000000 0.843234 1.000000 1.000000
[15,] 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
[16,] 0.843234 1.000000 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000
[17,] 1.000000 1.000000 0.843234 0.843234 1.000000 1.000000 0.843234 0.843234
[18,] 1.000000 1.000000 1.000000 1.000000 0.843234 1.000000 1.000000 1.000000
[19,] 1.000000 0.843234 0.843234 1.000000 1.000000 1.000000 1.000000 0.843234
[20,] 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]
 [1,] 0.843234 0.843234 1.000000 1.000000 1.000000 1.000000 0.843234 0.843234
 [2,] 1.000000 0.843234 1.000000 1.000000 1.000000 0.843234 1.000000 1.000000
 [3,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234
 [4,] 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234 1.000000 1.000000
 [5,] 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
 [6,] 1.000000 1.000000 1.000000 0.843234 1.000000 0.843234 1.000000 1.000000
 [7,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
 [8,] 1.000000 1.000000 1.000000 0.843234 0.843234 1.000000 1.000000 1.000000
 [9,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
[10,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234 1.000000
[11,] 1.000000 1.000000 1.000000 1.000000 0.843234 1.000000 0.843234 1.000000
[12,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234
[13,] 1.000000 1.000000 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000
[14,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234
[15,] 1.000000 0.843234 0.843234 1.000000 1.000000 1.000000 1.000000 1.000000
[16,] 1.000000 1.000000 1.000000 0.843234 1.000000 0.843234 1.000000 1.000000
[17,] 1.000000 1.000000 0.843234 0.843234 0.843234 1.000000 1.000000 1.000000
[18,] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
[19,] 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234 1.000000 1.000000
[20,] 1.000000 1.000000 1.000000 1.000000 1.000000 0.843234 1.000000 0.843234
         [,17]    [,18]    [,19]    [,20]
 [1,] 1.000000 1.000000 1.000000 0.843234
 [2,] 1.000000 1.000000 0.843234 1.000000
 [3,] 0.843234 1.000000 0.843234 1.000000
 [4,] 0.843234 1.000000 1.000000 1.000000
 [5,] 1.000000 0.843234 1.000000 1.000000
 [6,] 1.000000 1.000000 1.000000 1.000000
 [7,] 0.843234 1.000000 1.000000 1.000000
 [8,] 0.843234 1.000000 0.843234 1.000000
 [9,] 1.000000 1.000000 1.000000 1.000000
[10,] 1.000000 1.000000 1.000000 1.000000
[11,] 0.843234 1.000000 1.000000 1.000000
[12,] 0.843234 1.000000 1.000000 1.000000
[13,] 0.843234 1.000000 1.000000 1.000000
[14,] 1.000000 1.000000 0.843234 0.843234
[15,] 1.000000 1.000000 1.000000 1.000000
[16,] 1.000000 1.000000 1.000000 0.843234
[17,] 1.000000 0.843234 0.843234 1.000000
[18,] 0.843234 1.000000 0.843234 1.000000
[19,] 0.843234 0.843234 1.000000 1.000000
[20,] 1.000000 1.000000 1.000000 1.000000
#+end_example


#+begin_src R :session example :results output
print(Bo)
#+end_src

#+RESULTS[bad3b48f1882200a41147309b6b523c317c6e071]:
#+begin_example
20 x 20 sparse Matrix of class "dgCMatrix"

 [1,]  .        -0.156766  .         .         .        -0.156766  .
 [2,] -0.156766  .         .         .        -0.156766  .         .
 [3,]  .         .         .         .         .         .         .
 [4,]  .         .         .         .         .         .         .
 [5,]  .        -0.156766  .         .         .         .         .
 [6,] -0.156766  .         .         .         .         .         .
 [7,]  .         .         .         .         .         .         .
 [8,]  .         .         .        -0.156766  .        -0.156766  .
 [9,] -0.156766  .         .         .        -0.156766  .         .
[10,] -0.156766 -0.156766  .         .         .         .         .
[11,]  .         .         .         .         .         .         .
[12,]  .         .         .         .         .        -0.156766  .
[13,]  .         .         .         .         .         .         .
[14,]  .        -0.156766  .        -0.156766  .        -0.156766  .
[15,] -0.156766  .         .         .         .         .         .
[16,] -0.156766  .        -0.156766  .         .         .         .
[17,]  .         .        -0.156766 -0.156766  .         .        -0.156766
[18,]  .         .         .         .        -0.156766  .         .
[19,]  .        -0.156766 -0.156766  .         .         .         .
[20,] -0.156766  .         .         .         .         .         .

 [1,]  .        -0.156766 -0.156766  .         .         .         .
 [2,]  .         .        -0.156766  .         .         .        -0.156766
 [3,]  .         .         .         .         .         .         .
 [4,] -0.156766  .         .         .         .         .        -0.156766
 [5,]  .        -0.156766  .         .         .         .         .
 [6,] -0.156766  .         .         .        -0.156766  .        -0.156766
 [7,]  .         .         .         .         .         .         .
 [8,]  .         .         .         .        -0.156766 -0.156766  .
 [9,]  .         .         .         .         .         .         .
[10,]  .         .         .         .         .         .         .
[11,]  .         .         .         .         .        -0.156766  .
[12,] -0.156766  .         .         .         .         .         .
[13,] -0.156766  .         .        -0.156766  .         .         .
[14,]  .         .         .         .         .         .         .
[15,]  .         .        -0.156766 -0.156766  .         .         .
[16,]  .         .         .         .        -0.156766  .        -0.156766
[17,] -0.156766  .         .        -0.156766 -0.156766 -0.156766  .
[18,]  .         .         .         .         .         .         .
[19,] -0.156766  .         .         .         .         .        -0.156766
[20,]  .         .         .         .         .         .        -0.156766

 [1,] -0.156766 -0.156766  .         .         .        -0.156766
 [2,]  .         .         .         .        -0.156766  .
 [3,]  .        -0.156766 -0.156766  .        -0.156766  .
 [4,]  .         .        -0.156766  .         .         .
 [5,]  .         .         .        -0.156766  .         .
 [6,]  .         .         .         .         .         .
 [7,]  .         .        -0.156766  .         .         .
 [8,]  .         .        -0.156766  .        -0.156766  .
 [9,]  .         .         .         .         .         .
[10,] -0.156766  .         .         .         .         .
[11,] -0.156766  .        -0.156766  .         .         .
[12,]  .        -0.156766 -0.156766  .         .         .
[13,]  .         .        -0.156766  .         .         .
[14,]  .        -0.156766  .         .        -0.156766 -0.156766
[15,]  .         .         .         .         .         .
[16,]  .         .         .         .         .        -0.156766
[17,]  .         .         .        -0.156766 -0.156766  .
[18,]  .         .        -0.156766  .        -0.156766  .
[19,]  .         .        -0.156766 -0.156766  .         .
[20,]  .        -0.156766  .         .         .         .
#+end_example

****** Solve the Scaling Matrix
We don't need to worry about any terms of $\delta_{\mathbf{B}} = \mathtt{colsums\left(B\_o\right)+n}$ being 0:

#+begin_src R :session example
(δB   <- 1/(colSums(Bo)+n))
#+end_src

#+RESULTS[4e9078e02cac88e2b390d1de64aa94cfe1ea0faa]:
:  [1] 0.05290267 0.05203951 0.05120406 0.05120406 0.05120406 0.05161840
:  [7] 0.05039501 0.05246754 0.05079631 0.05120406 0.05120406 0.05161840
: [13] 0.05120406 0.05246754 0.05120406 0.05203951 0.05379495 0.05120406
: [19] 0.05246754 0.05120406

#+begin_src R :session example
(δB   <- 1/(colSums(B)))
#+end_src

#+RESULTS[dad08f4612601febd1e915b158e85aad0430943b]:
:  [1] 0.05290267 0.05203951 0.05120406 0.05120406 0.05120406 0.05161840
:  [7] 0.05039501 0.05246754 0.05079631 0.05120406 0.05120406 0.05161840
: [13] 0.05120406 0.05246754 0.05120406 0.05203951 0.05379495 0.05120406
: [19] 0.05246754 0.05120406


****** Find the Transition Probability Matrix
#+begin_src R :session example
  DB   <- diag(δB)
## ** Create the Transition Probability Matrix
## Create the Trans Prob Mat using Power Walk
  T <- Bo %*% DB
#+END_SRC

****** Implement the Loop
#+begin_src R :session example
## ** Implement the Power Walk
## *** Set Initial Values
  p_new  <- rep(1/n, n)  # Uniform
  p      <- rep(0, n)    # Zero
  η      <- 10^(-6)
## *** Implement the Loop

 while (sum(abs(p_new - p)) > η) {
    (p <- as.vector(p_new)) # P should remain a vector
    sum(p <- as.vector(p_new)) # P should remain a vector
     p_new  <- T %*% p + rep(t(δB) %*% p, n)
  }
## ** Report the Values
print(paste("The stationary point is"))
print(p)
#+end_src

#+RESULTS[95b35a53d822393ea1522a4e5bc714a0cd0834e2]:
: [1] "The stationary point is"
:  [1] 0.04882572 0.04963556 0.05044542 0.05044541 0.05044543 0.05004049
:  [7] 0.05125527 0.04923064 0.05085035 0.05044543 0.05044542 0.05004049
: [13] 0.05044542 0.04923064 0.05044543 0.04963557 0.04801586 0.05044542
: [19] 0.04923063 0.05044542
* Report Notes
- I could wrap ~listings~ in a ~tcolorbox~ environment just like I did for minted
  + I could also use shadowbox with ~listings~ for a similar aesthetics, but
    then I'd have to use ~sed~ to make them floats after ~org~ export.
- The ~tcolorbox~ may work as a float without effort?
- I know that ~listings~ has an option to float over two columns, maybe that's what I should do, or just avoid two columns all toghether.
* Relating the Power Walk to the Random Surfer
  :PROPERTIES:
  :CUSTOM_ID: relating-terms-in-power-walk-to-random-surfer
  :END:
** Introduction
These are notes relating to [[cite:parkPowerWalkRevisiting2013][\textsection 3.3]]

So if a term in the Power Walk can be related to $\alpha$ in the random
surfer, which is in turn $\xi_2$, I'll be able to understand it better. [fn:: Although I'm not quite sure why $\alpha$ is $\xi_{2}$ either]

Consider the equation:


\begin{align*}
\mathbf{T}&= \mathbf{B}\mathbf{D}_{\mathbf{B}}^{- 1} \\
&= \left( \mathbf{B}+  \mathbf{O} - \mathbf{O} \right) \mathbf{D}_{\mathbf{B}}^{- 1} \\
\end{align*}


Break this into to terms so that we can simplify it a bit:


\begin{align*}
    \mathbf{T} &= \Bigg[ \left( \mathbf{B}- \mathbf{O} \right)\mathbf{D}_{\mathbf{B}}^{- 1} \Bigg] + \Bigg\{  \mathbf{O}\mathbf{D}_{\mathbf{B}}^{- 1} \Bigg\}
\end{align*}
** Value of [1st Term]
   :PROPERTIES:
   :CUSTOM_ID: value-of-1st-term
   :END:

Observe that for all $\forall i,j\in \mathbb{Z}^+$:


\begin{align*}
\mathbf{A}_{i, j} \in \left\{0, 1\right\} \\
\implies  \mathbf{B}^{\mathbf{A}_{i, j}} &\in \left\{\beta^0, \beta^1\right\} \\
                     &= \left\{1, \beta \right\}  \\
                      \implies  \beta \mathbf{A} = \left\{1, \beta \right\}
\end{align*}


Using this property we get the following


\begin{align*}
\mathbf{B}_{i,j}- \mathbf{O}_{i,j} = \left( \beta^{\mathbf{A}_{i,j}} -1 \right) &=
\begin{cases}
    0      , &\enspace \mathbf{A}_{i,j}=0  \\
    \beta-1, &\enspace \mathbf{A}_{i,j}=1  \\
\end{cases} \\
\left( \beta- 1 \right) \mathbf{A}_{i,j} &=
\begin{cases}
    0      , &\enspace \mathbf{A}_{i,j}=0  \\
    \beta-1, &\enspace \mathbf{A}_{i,j}=1  \\
\end{cases} \\
\end{align*}


This means we have


\begin{align*}
\mathbf{A} \in \left\{0, 1\right\} \forall i,j  \implies   \mathbf{B}_{i,j}- \mathbf{O}_{i,j} &= \left( \beta-1 \right) \mathbf{A}_{i,j}
\end{align*}



\begin{align*}
\mathbf{B}&= \left( \mathbf{B}+  \mathbf{O}- \mathbf{O} \right) \\
&= \left( \mathbf{B}- 1 \right)
\end{align*}

** Value of {2nd Term}
  :PROPERTIES:
  :CUSTOM_ID: value-of-2nd-term
  :END:


\begin{align*}
\mathbf{O} \mathbf{D_B^{- 1}} &=
\begin{pmatrix}
    1 & 1      & 1 &        \\
    1 & 1      & 1 &\cdots  \\
    1 & 1      & 1 &        \\
      & \vdots &   &\ddots
\end{pmatrix}
\begin{pmatrix}
    \frac{1}{\delta_1} & 1                    & 1                   & \\
    1                  & \frac{1}{\delta_{2}} & 1 \cdots            & \\
    1                  & 1                    &  \frac{1}{\delta_3} & \\
               & \vdots &             &                     \ddots
\end{pmatrix}
\\
&= n
\begin{pmatrix}
    \frac{1}{n} & \frac{1}{n}      & \frac{1}{n} &        \\
    \frac{1}{n} & \frac{1}{n}      & \frac{1}{n} &\cdots  \\
    \frac{1}{n} & \frac{1}{n}      & \frac{1}{n} &        \\
      & \vdots &   &\ddots
\end{pmatrix}
\begin{pmatrix}
    \frac{1}{\delta_1} & 1                    & 1                   &        \\
    1                  & \frac{1}{\delta_2}    & 1                   & \cdots \\
    1                  & 1                    &  \frac{1}{\delta_3} &        \\
                       & \vdots               &                     & \ddots
\end{pmatrix}
\\
&= n \mathbf{E}\mathbf{D_B}^{-1}
\end{align*}


where the following definitions hold ($\forall i, j \in \mathbb{Z}^+$):

- $\mathbf{E}_{i, j} = \frac{1}{n}$
- $\mathbf{D_B}^{-1}_{k, k} = \frac{1}{\delta_k}$
- The value of $\delta$ is value that each term in a column must be
  divided by to become zero, in the case of the power walk that is just
  $\frac{1}{\mathtt{colSums}\left( \mathbf{B} \right)} = \vec{1}\mathbf{B}$,
  but if there were zeros in a column, it would be necessary to swap out
  the $0$s for $1$s and then sum in order to prevent a division by zero
  issue and because the 0s should be left.
- $\mathbf{A}\in \left\{0, 1\right\} \forall i,j$ is the unweighted
  adjacency matrix of the relevant graph.

putting this all together we can do the following:


\begin{align*}
\mathbf{T}&= \mathbf{B}\mathbf{D}^{- 1}_{\mathbf{B}} \\
&= \left( \mathbf{B}+  \mathbf{O} - \mathbf{O} \right) \mathbf{D}_{\mathbf{B}}^{- 1} \\
&= \left( \mathbf{B}- \mathbf{O} \right)\mathbf{D}_{B}^{- 1}  +  \mathbf{O} {\mathbf{D}_{\mathbf{B}}^{- 1}} \\
 \intertext{From above:} \\
&= \left( \beta- 1 \right) \mathbf{A}_{i,j} +  n \mathbf{E} \mathbf{D}_{\mathbf{B}}^{- 1}\\
&= \mathbf{A}_{i,j}\left( \beta- 1 \right)  +  n \mathbf{E} \mathbf{D}_{\mathbf{B}}^{- 1}\\
 \intertext{because $\mathbf{D} \mathbf{D}^{- 1} = \mathbf{I}$ we can multiply one side through:} \\
&= \mathbf{D}_{\mathbf{A}} \mathbf{D}_{\mathbf{A}}^{- 1}\mathbf{A}_{i,j}\left( \beta- 1 \right)  +  n \mathbf{E} \mathbf{D}_{\mathbf{B}}^{- 1}\\
\end{align*}


But the next step requires showing that:


\begin{align*}
\left( \beta-1 \right)\mathbf{D}_\mathbf{A} \mathbf{D}_{\mathbf{B}}^{- 1} &= \mathbf{I} - n \mathbf{D}_{B}^{- 1}
\end{align*}

** Equate the Power Walk to the Random Surfer
Define the matrix $\mathbf{D}_{\mathbf{M}}$:

\begin{align}
    \mathbf{D}_{\mathbf{M}} = \mathrm{diag}\left( \mathtt{colSum} \left( \mathbf{M} \right) \right) &= \mathrm{diag} \left( \vec{1} \mathbf{M} \right)
\end{align}


To scale each column of that matrix to 1, each column will need to be divieded by the column sum, unless the column is already zero, this needs to be done to turn an adjacency matrix into a matrix of probabilities:

\begin{align}
    \mathbf{D}_{\mathbf{A}} ^{- 1} :  \left[     \mathbf{D}_{\mathbf{A}} ^{- 1}  \right]_i =
    \begin{cases}
	0 ,& \quad \left[ \mathbf{D}_{\mathbf{A}} \right]_i = 0 \\
	\left[ \frac{1}{\mathbf{D}_{\mathbf{A}}} \right] ,& \enspace \enspace \left[ \mathbf{D}_{\mathbf{A}} \right]_i \neq 0
    \end{cases}
\end{align}

In the case of the power walk $\mathbf{B}= \beta^{\mathbf{A}} \neq 0$ so it is sufficient:

\begin{align}
    \mathbf{D}_{\mathbf{B}}^{- 1} &= \frac{1}{\mathrm{diag}\left( \vec{1} \left(\mathbf{\beta^{\mathbf{A}}  \right) } \right)}
\end{align}


Recall that the /power walk/ gives a transition probability matrix:

\begin{align}
%    \mathbf{T} &= \mathbf{a} \text{\fboxsep=.2em\fbox{$x$}} \\
    \text{\textbf{Power Walk}} \nonumber \\
\mathbf{T} &= \text{\fboxsep=.2em\fbox{$\mathbf{A}\mathbf{D}_{\mathbf{A}}^{- 1}$}}  \mathbf{D}_{\mathbf{A}} \left( \beta - 1 \right) \mathbf{D}_{\mathbf{B}}^{- 1} + \text{\fboxsep=.2em\fbox{$\mathbf{E}$}} n \mathbf{D}_{\mathbf{B}}^{- 1}  \label{eq:pwbx}\\
    \text{\textbf{Random Surfer}} \nonumber \\
    \mathbf{T} &= \alpha \text{\fboxsep=.2em\fbox{$\mathbf{A}\mathbf{D}_{\mathbf{A}}^{- 1}$}}  + \left( 1-\alpha \right) \text{\fboxsep=.2em\fbox{$\mathbf{E}$}}
\end{align}

So these are equivalent when:

\begin{align}
\mathbf{D}_{\mathbf{A}}   \left( \beta -  1 \right)\mathbf{D}_{\mathbf{B}^{- 1}} &=\mathbf{I}  \alpha \label{fl} \\
    \ \nonumber \\
  \vec{1}  \left( 1- \alpha \right) &=  - n \mathbf{D}_{\mathbf{B}}^{- 1}  \nonumber \\
    \implies  \vec{1}\alpha &=  \vec{1}- n \mathbf{D}_{\mathbf{B}}^{- 1} \label{st} \\
    \intertext{Hence we have:} \notag \\
\mathbf{D}_{\mathbf{A}}  \left( \beta -  1 \right)\mathbf{D}_{\mathbf{B}}^{- 1} &=  \vec{1}\alpha =  \mathbf{I}- n \mathbf{D}_{\mathbf{B}}^{- 1} \label{eq:eqalpha}
\end{align}


Solving for $\beta$  with eqref:fl :

\begin{align}
    \beta&= \frac{1- \Theta}{\Theta}\\
%    \beta&= \frac{\alpha - \mathbf{D}_{\mathbf{A}}\mathbf{D}_{\mathbf{B}}^{- 1}}{\mathbf{D}_{\mathbf{A}}\mathbf{D}_{\mathbf{B}}^{-1}}
\end{align}

where: [fn:bvl]

- $\Theta = \mathbf{D}_{\mathbf{A}} \mathbf{D}_{\mathbf{B}}^{- 1}$

but we can't really do this so instead:

\[
\beta \mathbf{1}_{\tiny \left[ n,n \right]}  = \left( 1 - \Theta \right) \Theta^{-1} \label{eq:betadef}
\]

If $\beta$ is set accordingly then by eqref:eq:eqalpha:

\begin{align}
    \mathbf{A}\left( \beta- 1 \right) \mathbf{D}_{\mathbf{B}}^{- 1} &= \alpha = \mathbf{I}- n \mathbf{D}_{\mathbf{B}}^{- 1} \nonumber \\
     \implies  \mathbf{A}\left( \beta- 1 \right) \mathbf{D}_{\mathbf{B}}^{- 1} &=  \mathbf{I}- n \mathbf{D}_{\mathbf{B}}^{- 1}
\end{align}

And setting $\Gamma = \mathbf{I}- n \mathbf{D}_{\mathbf{B}}^{- 1}$  from eqref:st and putting in \eqref{eq:pwbx} we have:

\begin{align}
\mathbf{T} &= \text{\fboxsep=.2em\fbox{$\mathbf{A}\mathbf{D}_{\mathbf{A}}^{- 1}$}}  \mathbf{D}_{\mathbf{A}} \left( \beta - 1 \right) \mathbf{D}_{\mathbf{B}}^{- 1} + \text{\fboxsep=.2em\fbox{$\mathbf{E}$}} n \mathbf{D}_{\mathbf{B}}^{- 1}  \nonumber \\
  \mathbf{T} &= \Gamma \text{\fboxsep=.2em\fbox{$\mathbf{A}\mathbf{D}_{\mathbf{A}}^{- 1}$}}  + \left( 1-\Gamma \right) \text{\fboxsep=.2em\fbox{$\mathbf{E}$}} \nonumber \\
  \ \nonumber \\
  \mathbf{T} &= \Gamma \mathbf{A}\mathbf{D}_{\mathbf{A}}^{- 1}  + \left( 1-\Gamma \right) \mathbf{E}
  \end{align}

  Where $\mathbf{E}$ is square matrix of $\frac{1}{n}$ as in eqref:eq:bgval1  eqref:eq:bgVal2

** Conclusion
So when the adjacency matrix is stictly boolean, the power walk is equivalent to the random surfer.

** TODO The Second Eigenvalue
*** TODO The Random Surfer
The Second eigenvalue \(\xi_2\) of the Power Surfer is less than $\alpha$ ([[file:Proposal/Propsal.org::#stability-convergence][See 3.2; Stability and Concvergence, of proposal]]).
*** TODO Power Walk
Because the Power Walk relates to the random surfer as demonstrated in section [[#relating-terms-in-power-walk-to-random-surfer]], what can be said about $\xi_{2}$
**** Applying this to Power Walk
Let $\Lambda_{\left( 2 \right)}\left( \mathbf{T} \right) = \lambda_2$ return the second value of a transition, probability Matrix, then observe that:


\begin{align}
    \Lambda_{\left( 2 \right)} \left( \mathbf{T}_{\text{\tiny RS}} \right)  \leq \left\lvert \alpha \right\rvert  \implies      \Lambda_{\left( 2 \right)} \left( \mathbf{T}_{\text{\tiny PW}} \right) \leq \left\lvert \frac{\alpha - \mathbf{D}_{\mathbf{a}} \mathbf{D}_{\mathbf{B}}^{- 1}}{\mathbf{D}_{\mathbf{A}}\mathbf{D}_{\mathbf{B}}^{-1}}  \right\rvert
\end{align}

where:


 - $\lambda_{\left( 2 \right)} \left( \mathbf{T} \right)$ refers to the transition probability matrix of the power walk and random surfer approaces as indicated.
***** My attempt
\begin{align}
    \beta \mathbf{1}_{\tiny \left[ n, n \right] }    &= \frac{1- \Theta}{\Theta} \label{eq:betasig}\\
%    \beta&= \frac{\alpha - \mathbf{D}_{\mathbf{A}}\mathbf{D}_{\mathbf{B}}^{- 1}}{\mathbf{D}_{\mathbf{A}}\mathbf{D}_{\mathbf{B}}^{-1}}
\end{align}

where:
- $\Theta = \mathbf{D}_{\mathbf{A}} \mathbf{D}_{\mathbf{B}}^{- 1}$

So I thought maybe if I could find a value of $\beta$ that satisfied eqref:eq:betasig then I could show circumstances under which $\left\lvert \xi_2 \right\rvert < \alpha$.

Seemingly it's only satisfied where $\beta = 1$ though, using this simulation:

#+begin_src R
g1 <- igraph::erdos.renyi.game(n = 9, 0.2)
A <- igraph::get.adjacency(g1) # Row to column
A <- t(A)
# plot(g1)

## * Finding beta values to behave like Random Surfer
  beta <- 10
  B <- beta^A

  DA     <- PageRank::create_sparse_diag_sc_inv_mat(A)
  DB_inv <- PageRank::create_sparse_diag_scaling_mat(B)

 THETA <- DA %*% DB_inv

THETA <- function(A, beta) {
  B  <- beta^A
  DA     <- PageRank::create_sparse_diag_sc_inv_mat(A)
  DB_inv <- PageRank::create_sparse_diag_scaling_mat(B)
  return(DA %*% DB_inv)
}

THETA_inv <- function(A, beta) {
  B  <- beta^A
  DB     <- PageRank::create_sparse_diag_sc_inv_mat(B)
  DA_inv <- PageRank::create_sparse_diag_scaling_mat(A)
  return(DA %*% DB_inv)
}

beta_func <- function(A, beta) {
    return(1-THETA(A, beta^A) %*% THETA_inv(A, beta^A))
}

THETA(A, 10) %*% THETA_inv(A, 10)


eta <- 10^-6
beta <- 1.01
while (mean(beta*matrix(1, nrow(A), ncol(A)) - beta_func(A, beta)) > eta) {
    beta <- beta + 0.01
    print(beta)
    print(diag(beta_func(A, beta)))
    print(beta*matrix(1, nrow(A), ncol(A)))
    print(beta_func(A, beta))
#    Sys.sleep(0.1)
}

beta


diag(beta_func(A, beta))
beta


## * blah
#+end_src
* Investigating the Second EigenValue

Maybe I should look at the most appropriate way to simulate social network links, one possibility is [[https://crpit.scem.westernsydney.edu.au/confpapers/CRPITV144Zeng.pdf][this paper ]] cite:zengPracticalSimulationMethod2013.

Actually there is a data set available
 cite:garritanoWikipediaArticleNetworks2019, I should just analyse that, see [[file:~/Dropbox/DataSci/Visual_Analytics/Assessment/the-marvel-universe-social-network/plotly3d_Marvel.r][how
it was done in Visual Analytics as a reminder]].

Using the Wikipedia ArtYeah I think that's right, thaicle compare density and Determinant.

Is the determinant easily calculated for a large matrix?
  It appears to diverge

  Will the determinant diverge for large matrices?
  Will the prob of making edges in the game just be the density?

  Look at comparing the determinant and the density of the wikipedia adjacency matrix.

  What are some ways that we can model the second eigenvalue?

** Plotting Various Values

There is some relationship between the determinant and the density

#+BEGIN_SRC R :exports both :results output graphics file :file Media/EigenValue_Determinant.png :eval never-export
  library(pacman)
  pacman::p_load(PageRank, devtools, Matrix, igraph, tidyverse)
n <- 20
p <- 1:n/n
beta <- 1:n/n
beta <- runif(n)*100
sz <- 1:n/n+10
input_var <- expand.grid("n" = n, "p" = p, "beta" = beta, "size" = sz)
input_var


random_graph <- function(n, p, beta, size) {
      g1 <- igraph::erdos.renyi.game(n = sz, p)
      A <- igraph::get.adjacency(g1) # Row to column
      A <- Matrix::t(A)

      A_dens <- mean(A)
      T      <- PageRank::power_walk_prob_trans(A)
      e2     <- eigen(T, only.values = TRUE)$values[2] # R orders by descending magnitude
      A_det  <- det(A)
      return(c(abs(e2), A_det))
}

## TODO this should use pmap.
Y <- matrix(ncol = 2, nrow = nrow(input_var))
for (i in 1:nrow(input_var)) {
  X <- as.vector(input_var[i,])
  Y[i,] <-  random_graph(X$n, X$p, X$beta, X$size)
}
if (sum(abs(Y) != abs(Re(Y))) == 0) {
  Y <- Re(Y)
}
nrow(input_var)
nrow(Y)
Y <- as.data.frame(Y); colnames(Y) <- c("eigenvalue2", "determinant")

data <- cbind(input_var, Y)

ggplot(data, aes(x = determinant, y = eigenvalue2, size = beta, color = size, shape = factor(n))) +
  geom_point() +
  labs(x = "Determinant of Adjacency Matrix", y = "Second Eigenvalue of Power Walk Transition Probability Matrix") +
  scale_size_continuous(range = c(0.1,1))
#+end_src

#+RESULTS[08882d661cf5d2410c8335d1850632709c7cf5c5]:
[[file:Media/EigenValue_Determinant.png]]


#+BEGIN_SRC R :exports both :results output graphics file :file Media/EigenValue_Density.png :eval never-export
  library(pacman)
  pacman::p_load(PageRank, devtools, Matrix, igraph, tidyverse)
n <- 100
p <- 1:n/n
beta <- 1:n/n
beta <- runif(n)*100
sz <- 1:n/n+10
input_var <- expand.grid("n" = n, "p" = p, "beta" = beta, "size" = sz)
input_var


random_graph <- function(n, p, beta, size) {
      g1 <- igraph::erdos.renyi.game(n = sz, p)
      A <- igraph::get.adjacency(g1) # Row to column
      A <- Matrix::t(A)

      A_dens <- mean(A)
      T      <- PageRank::power_walk_prob_trans(A)
      e2     <- eigen(T, only.values = TRUE)$values[2] # R orders by descending magnitude
      A_det  <- det(A)
      return(c(abs(e2), A_dens))
}

## TODO this should use pmap.
Y <- matrix(ncol = 2, nrow = nrow(input_var))
for (i in 1:nrow(input_var)) {
  X <- as.vector(input_var[i,])
  Y[i,] <-  random_graph(X$n, X$p, X$beta, X$size)
}
if (sum(abs(Y) != abs(Re(Y))) == 0) {
  Y <- Re(Y)
}
nrow(input_var)
nrow(Y)
Y <- as.data.frame(Y); colnames(Y) <- c("eigenvalue2", "determinant")

data <- cbind(input_var, Y)

ggplot(data, aes(x = determinant, y = eigenvalue2, color = size, shape = factor(n))) +
  geom_point(base_size = 99, aes(size = beta)) +
  labs(x = "Density of Adjacency Matrix", y = "Second Eigenvalue of Power Walk Transition Probability Matrix") +
  scale_size_continuous(range = c(0.1,1))
#+end_src

Maybe this looks like a Chi distribution?

#+BEGIN_SRC R :exports both :results output graphics file :file Media/EigenValue_Density_Chi.png :eval never-export
chival <- dchisq(seq(from = 0, to = 40, length.out = 100), df = 10)*6
index  <- seq(from = 0, to = 2, length.out = 100)
chidata  <- data.frame(index = index, chi = chival)
ggplot(data) +
  geom_point(mapping = aes(x = determinant, y = eigenvalue2, size = beta, color = size, shape = factor(n))) +
  geom_line(data = chidata, mapping = aes(x = index, y = chi)) +
  scale_size_continuous(range = c(0.1,1)) +
  labs(x = "Density of Adjacency Matrix", y = "Second Eigenvalue of Power Walk Transition Probability Matrix")
#+end_src

#+RESULTS[c7a830cfab9be72b1ce3782f148a5dcb92c49f48]:
[[file:Media/EigenValue_Density_Chi.png]]

** TODO Model the log transformed data using a linear regression or log(-x) regression

\begin{align}
    \xi_2 &= \left( 1-  \frac{\sum^{n}_{i= 1} \sum^{n}_{j= 1}   \mathbf{A}_{i,j}  }{n^{2}} \right)^{0.6} \cdot  e^{- 0.48} \pm \Delta
\end{align}

*** TODO Change the colour of each model by using pivot_longer
** TODO Could I get better performance by also considering the determinant?
No not really, it terms of accuracy

** TODO Is the determinant faster or slower?
Significantly slower for large matrices.
** TODO Import wikipedia data
- +Import the wikipedia data+
- +Measure the density+
- +Use the density to guess the \(p\) of the game+
  + +Justify the witht the scatterplot matrix+
- +Measure the affect of different \(\beta\) values on \(\lambda_2\) for graphs ov various sizes given that \(p\) value.+
  + +Or atleast a range within that prob+

    use a /Barabassi-Albert/ Random Graph through the ~igraph::

** TODO What about the trace of the matrix?
* What's the central article of Wikipedia

* Appendix
** Glossary
- Eigenvector Centrality :: PageRank
  - The probability of landing on a vertex in a random walk by adding a small random probability to each vertex.
- Irreducible :: Ergodic
  - All Vertices can be reached from any other vertex

* Footnotes

[fn:bvl] NOTE: Similar to a signmoid function, which is a solution to $p \propto p(1-p)$, I wonder if this provides a connection to the exponential nature of the power walk

[fn:un]
